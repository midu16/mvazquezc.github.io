[{"content":"Working with Pod Security Standards In Kubernetes v1.25 Pod Security admission has moved to stable, replacing Pod Security Policy admission. This feature has been in beta and enabled by default since Kubernetes v1.23 in this post we are going to cover what\u0026rsquo;s new with Pod Security Admission (PSA) and how it affects the workloads being deployed in our clusters.\nNote\nFor this post I\u0026rsquo;ll be running a Kubernetes v1.25 cluster. If you want to try this in your own environment you can use your favorite tool to get a K8s cluster up and running, I\u0026rsquo;ll be using kcli.\n# Create a Kubernetes 1.25 cluster with 1 master and 1 worker using calico as SDN, nginx as ingress controller, metallb for loadbalancer services and CRI-O as container runtime kcli create kube generic -P masters=1 -P workers=1 -P master_memory=4096 -P numcpus=2 -P worker_memory=4096 -P sdn=calico -P version=1.25 -P ingress=true -P ingress_method=nginx -P metallb=true -P engine=crio -P domain=linuxera.org psa-cluster This is how our cluster looks like:\nkubectl get nodes NAME STATUS ROLES AGE VERSION psa-cluster-master-0.linuxera.org Ready control-plane,master 4m19s v1.25.0 psa-cluster-worker-0.linuxera.org Ready worker 1m20s v1.25.0 Pod Security Admission The Pod Security Admission relies on both Pod Security Standards which define the different security policies that need to be checked for workloads and Pod Admission Modes that define how the standards are applied for a given namespace.\nPod Security Standards This new admission plugin relies on pre-backed Pod Security Standards. These standards will evolve every Kubernetes release to include / adapt new security rules.\nAs of Kubernetes v1.25 there are three Pod Security Standards defined:\nNote\nYou can read each standard requirements on this link.\nprivileged baseline restricted Pod Admission Modes The cluster admin/namespace admin can configure an admission mode that will be used to do admission validations against workloads being deployed in the namespace. There are three admission modes that can be configured on a namespace:\nenforce: Policy violations will cause the pod to be rejected. audit: Policy violations will be logged in the audit log, pod will be allowed. warn: Policy violations will cause a user-facing warning, pod will be allowed. Each mode can be configured with a different Pod Security Standard. For example, a namespace could enforce using the privileged standard and audit/warn via therestricted standard.\nThe admission modes and the standards to be used are configured at the namespace level via the use of the pod-security.kubernetes.io/\u0026lt;MODE\u0026gt;: \u0026lt;LEVEL\u0026gt; label.\nAs earlier mentioned, these Pod Security Standards will evolve over time, and since these are versioned we can specify which version of a specific mode we want to enforce via the use of the pod-security.kubernetes.io/\u0026lt;MODE\u0026gt;-version: \u0026lt;VERSION\u0026gt; label, where \u0026lt;VERSION\u0026gt; refers to a Kubernetes minor version like v1.25.\nIf we put all this information together, we can get to a namespace definition like the one below:\nNote\nIn the example below we use the version v1.25, a namespace could also point to the latest available by using latest instead.\napiVersion: v1 kind: Namespace metadata: name: test-namespace labels: pod-security.kubernetes.io/enforce: privileged pod-security.kubernetes.io/enforce-version: v1.25 pod-security.kubernetes.io/audit: restricted pod-security.kubernetes.io/audit-version: v1.25 pod-security.kubernetes.io/warn: restricted pod-security.kubernetes.io/warn-version: v1.25 It\u0026rsquo;s important to mention that audit and warning modes are applied to workload resources (resources that have a pod template definition) like Deployments, Jobs, etc. to help catch violations early. On the other hand, enforce mode is applied to the resulting pod object.\nPod Security Admission Configuration Pod Security Admission comes pre-configured in Kubernetes v1.25 with the least restrictive policy, it\u0026rsquo;s possible to modify the default configuration by modifying the admission configuration for this plugin, you can read here how to do it.\nIf you checked the link above, you have seen that exemptions can be configured for the admission, this will allow the cluster admin to configure users, runtime classes or namespaces that won\u0026rsquo;t be evaluated by PSA. From this three exemptions, the runtime class could be helpful if you want to keep a namespace as restrictive as possible by default, but then have some workload that is not evaluated against a PSA.\nPod Security Standards in Action Now that we know the basics around PSA, we can go ahead and run some tests to understand how it works. We will be using a simple go app.\nNon-restrictive namespace In this first example we\u0026rsquo;re going to deploy our workload in a namespace that enforces the privileged standard and audits/warns the restricted standard.\nCreate the namespace for our workload with the appropriated PSA settings:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: non-restrictive-namespace labels: pod-security.kubernetes.io/enforce: privileged pod-security.kubernetes.io/enforce-version: v1.25 pod-security.kubernetes.io/audit: restricted pod-security.kubernetes.io/audit-version: v1.25 pod-security.kubernetes.io/warn: restricted pod-security.kubernetes.io/warn-version: v1.25 EOF Create the workload:\ncat \u0026lt;\u0026lt;EOF | kubectl -n non-restrictive-namespace apply -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: go-app name: go-app spec: replicas: 1 selector: matchLabels: app: go-app strategy: {} template: metadata: labels: app: go-app spec: containers: - image: quay.io/mavazque/reversewords:latest name: reversewords resources: {} EOF We got some client warnings (caused by the warn mode) saying the violations of our workload when checked against the restricted standard:\nWarning: would violate PodSecurity \u0026#34;restricted:v1.25\u0026#34;: allowPrivilegeEscalation != false (container \u0026#34;reversewords\u0026#34; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \u0026#34;reversewords\u0026#34; must set securityContext.capabilities.drop=[\u0026#34;ALL\u0026#34;]), runAsNonRoot != true (pod or container \u0026#34;reversewords\u0026#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \u0026#34;reversewords\u0026#34; must set securityContext.seccompProfile.type to \u0026#34;RuntimeDefault\u0026#34; or \u0026#34;Localhost\u0026#34;) But the workload is running:\nkubectl -n non-restrictive-namespace get pod NAME READY STATUS RESTARTS AGE go-app-5b954b7b74-kwkwn 1/1 Running 0 1m30s In the next scenario we will configure the enforce mode to the restricted standard.\nRestrictive namespace Create the namespace for our workload with the appropriated PSA settings:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: restrictive-namespace labels: pod-security.kubernetes.io/enforce: restricted pod-security.kubernetes.io/enforce-version: v1.25 pod-security.kubernetes.io/audit: restricted pod-security.kubernetes.io/audit-version: v1.25 pod-security.kubernetes.io/warn: restricted pod-security.kubernetes.io/warn-version: v1.25 EOF Create the workload:\ncat \u0026lt;\u0026lt;EOF | kubectl -n restrictive-namespace apply -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: go-app name: go-app spec: replicas: 1 selector: matchLabels: app: go-app strategy: {} template: metadata: labels: app: go-app spec: containers: - image: quay.io/mavazque/reversewords:latest name: reversewords resources: {} EOF Again, we got some client warnings (caused by the warn mode) saying the violations of our workload when checked against the restricted standard:\nWarning: would violate PodSecurity \u0026#34;restricted:v1.25\u0026#34;: allowPrivilegeEscalation != false (container \u0026#34;reversewords\u0026#34; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \u0026#34;reversewords\u0026#34; must set securityContext.capabilities.drop=[\u0026#34;ALL\u0026#34;]), runAsNonRoot != true (pod or container \u0026#34;reversewords\u0026#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \u0026#34;reversewords\u0026#34; must set securityContext.seccompProfile.type to \u0026#34;RuntimeDefault\u0026#34; or \u0026#34;Localhost\u0026#34;) And this time, the workload is NOT running:\nkubectl -n restrictive-namespace get pod No resources found in restrictive-namespace namespace. If you remember, the enforce mode is applied against the pod object and not against the workload objects (like Deployment in this case). That\u0026rsquo;s why the deployment was admitted but the pod it\u0026rsquo;s not.\nWe can see in the namespace events / replicaset status why the pod is not running:\nkubectl -n restrictive-namespace get events LAST SEEN TYPE REASON OBJECT MESSAGE 3m44s Warning FailedCreate replicaset/go-app-5b954b7b74 Error creating: pods \u0026#34;go-app-5b954b7b74-dfq9g\u0026#34; is forbidden: violates PodSecurity \u0026#34;restricted:v1.25\u0026#34;: allowPrivilegeEscalation != false (container \u0026#34;reversewords\u0026#34; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \u0026#34;reversewords\u0026#34; must set securityContext.capabilities.drop=[\u0026#34;ALL\u0026#34;]), runAsNonRoot != true (pod or container \u0026#34;reversewords\u0026#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \u0026#34;reversewords\u0026#34; must set securityContext.seccompProfile.type to \u0026#34;RuntimeDefault\u0026#34; or \u0026#34;Localhost\u0026#34;) If we want this workload to be admitted in the cluster we need to fine tune the pod\u0026rsquo;s configuration, let\u0026rsquo;s remove the deployment and get it created with a config allowed by the restricted standard.\nRemove the deployment\nkubectl -n restrictive-namespace delete deployment go-app Create the workload with the proper config:\ncat \u0026lt;\u0026lt;EOF | kubectl -n restrictive-namespace apply -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: go-app name: go-app spec: replicas: 1 selector: matchLabels: app: go-app strategy: {} template: metadata: labels: app: go-app spec: containers: - image: quay.io/mavazque/reversewords:latest name: reversewords resources: {} securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL runAsNonRoot: true runAsUser: 1024 seccompProfile: type: RuntimeDefault EOF This time we didn\u0026rsquo;t get any warnings and if we check for pods in the namespace we will see our workload is running:\nkubectl -n restrictive-namespace get pod NAME READY STATUS RESTARTS AGE go-app-5f45c655b6-z26kv 1/1 Running 0 25s Tip 1 - Check if a given workload would be rejected in a given namespace You can try to create a workload against a given namespace in dry-run mode and get client warnings, example:\ncat \u0026lt;\u0026lt;EOF | kubectl -n restrictive-namespace apply --dry-run=server -f - apiVersion: apps/v1 kind: Deployment metadata: labels: app: go-app name: go-app spec: replicas: 1 selector: matchLabels: app: go-app strategy: {} template: metadata: labels: app: go-app spec: containers: - image: quay.io/mavazque/reversewords:latest name: reversewords resources: {} EOF You will get a warning like this:\nWarning: would violate PodSecurity \u0026#34;restricted:v1.25\u0026#34;: allowPrivilegeEscalation != false (container \u0026#34;reversewords\u0026#34; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \u0026#34;reversewords\u0026#34; must set securityContext.capabilities.drop=[\u0026#34;ALL\u0026#34;]), runAsNonRoot != true (pod or container \u0026#34;reversewords\u0026#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \u0026#34;reversewords\u0026#34; must set securityContext.seccompProfile.type to \u0026#34;RuntimeDefault\u0026#34; or \u0026#34;Localhost\u0026#34;) deployment.apps/go-app created (server dry run) Tip 2 - Check if workloads on a given namespace would violate a given policy You can try to label a namespace in dry-run mode and get client warnings, as an example let\u0026rsquo;s see what would happen if we moved the namespace from the first scenario from the privileged standard to the restricted one:\nkubectl label --dry-run=server --overwrite ns non-restrictive-namespace pod-security.kubernetes.io/enforce=restricted You will get a warning like this:\nWarning: existing pods in namespace \u0026#34;non-restrictive-namespace\u0026#34; violate the new PodSecurity enforce level \u0026#34;restricted:v1.25\u0026#34; Warning: go-app-5b954b7b74-kwkwn: allowPrivilegeEscalation != false, unrestricted capabilities, runAsNonRoot != true, seccompProfile namespace/non-restrictive-namespace labeled Closing Thoughts Pod Security Admission is a great addition to the Kubernetes security, I hope this time its adoption increases compared to PSPs. In the next post we will talk about the new changes around Seccomp that were introduced in Kubernetes.\n","permalink":"https://linuxera.org/working-with-pod-security-standards/","summary":"Working with Pod Security Standards In Kubernetes v1.25 Pod Security admission has moved to stable, replacing Pod Security Policy admission. This feature has been in beta and enabled by default since Kubernetes v1.23 in this post we are going to cover what\u0026rsquo;s new with Pod Security Admission (PSA) and how it affects the workloads being deployed in our clusters.\nNote\nFor this post I\u0026rsquo;ll be running a Kubernetes v1.25 cluster. If you want to try this in your own environment you can use your favorite tool to get a K8s cluster up and running, I\u0026rsquo;ll be using kcli.","title":"Working with Pod Security Standards"},{"content":"Capabilities and Seccomp Profiles on Kubernetes In a previous post we talked about Linux Capabilities and Secure Compute Profiles, in this post we are going to see how we can leverage them on Kubernetes.\nWe will need a Kubernetes cluster, I\u0026rsquo;m going to use kcli in order to get one. Below command will deploy a Kubernetes cluster on VMs:\nNOTE: You can create a parameters file with the cluster configuration as well.\n# Create a Kubernetes 1.20 cluster with 1 master and 1 worker using calico as SDN, nginx as ingress controller, metallb for loadbalancer services and CRI-O as container runtime kcli create kube generic -P masters=1 -P workers=1 -P master_memory=4096 -P numcpus=2 -P worker_memory=4096 -P sdn=calico -P version=1.20 -P ingress=true -P ingress_method=nginx -P metallb=true -P engine=crio -P domain=linuxera.org caps-cluster After a few moments we will get the kubeconfig for accessing our cluster:\nKubernetes cluster caps-cluster deployed!!! INFO export KUBECONFIG=$HOME/.kcli/clusters/caps-cluster/auth/kubeconfig INFO export PATH=$PWD:$PATH We can start using it right away:\nexport KUBECONFIG=$HOME/.kcli/clusters/caps-cluster/auth/kubeconfig kubectl get nodes NAME STATUS ROLES AGE VERSION caps-cluster-master-0.linuxera.org Ready control-plane,master 8m19s v1.20.5 caps-cluster-worker-0.linuxera.org Ready worker 3m33s v1.20.5 Capabilities on Kubernetes Capabilities on Kubernetes are configured for pods or containers via the SecurityContext.\nIn the next scenarios we are going to see how we can configure different capabilities for our containers and how they behave depending on the user running our container.\nWe will be using a demo application that listens on a given port, by default the application image uses a non-root user. In a previous post we mentioned how capabilities behave differently depending on the user that runs the process, we will see how that affects when running on containers.\nContainer Runtime Default Capabilities As previously mentioned, container runtimes come with a set of enabled capabilities that will be assigned to every container if not otherwise specified. We\u0026rsquo;re using CRI-O in our Kubernetes cluster and we can find the default capabilities in the CRI-O configuration file at /etc/crio/crio.conf present in the nodes:\ndefault_capabilities = [ \u0026#34;CHOWN\u0026#34;, \u0026#34;DAC_OVERRIDE\u0026#34;, \u0026#34;FSETID\u0026#34;, \u0026#34;FOWNER\u0026#34;, \u0026#34;SETGID\u0026#34;, \u0026#34;SETUID\u0026#34;, \u0026#34;SETPCAP\u0026#34;, \u0026#34;NET_BIND_SERVICE\u0026#34;, \u0026#34;KILL\u0026#34;, ] The capabilities in the list above will be the ones added to containers by default.\nPod running with root UID\nCreate a namespace:\nNAMESPACE=test-capabilities kubectl create ns ${NAMESPACE} Create a pod running our test application with UID 0:\ncat \u0026lt;\u0026lt;EOF | kubectl -n ${NAMESPACE} create -f - apiVersion: v1 kind: Pod metadata: name: reversewords-app-captest-root spec: containers: - image: quay.io/mavazque/reversewords:ubi8 name: reversewords securityContext: runAsUser: 0 dnsPolicy: ClusterFirst restartPolicy: Never status: {} EOF Review the capability sets for the application process:\nkubectl -n ${NAMESPACE} exec -ti reversewords-app-captest-root -- grep Cap /proc/1/status CapInh:\t00000000000005fb CapPrm:\t00000000000005fb CapEff:\t00000000000005fb CapBnd:\t00000000000005fb CapAmb:\t0000000000000000 If we decode the effective set this is what we get:\ncapsh --decode=00000000000005fb NOTE: You can see how the pod got assigned the runtime\u0026rsquo;s default caps.\n0x00000000000005fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service Pod running with non-root UID\nCreate a pod running our test application with a non-root UID:\nNAMESPACE=test-capabilities cat \u0026lt;\u0026lt;EOF | kubectl -n ${NAMESPACE} create -f - apiVersion: v1 kind: Pod metadata: name: reversewords-app-captest-nonroot spec: containers: - image: quay.io/mavazque/reversewords:ubi8 name: reversewords securityContext: runAsUser: 1024 dnsPolicy: ClusterFirst restartPolicy: Never status: {} EOF Review the capability sets for the application process:\nkubectl -n ${NAMESPACE} exec -ti reversewords-app-captest-nonroot -- grep Cap /proc/1/status CapInh:\t00000000000005fb CapPrm:\t0000000000000000 CapEff:\t0000000000000000 CapBnd:\t00000000000005fb CapAmb:\t0000000000000000 You can see how the effective and permitted sets were cleared. We explained that behaviour in our previous post. That happens because we\u0026rsquo;re doing execve to an unprivileged process so those capability sets get cleared.\nThis has some consequences when running our workloads on Kubernetes, outside Kubernetes we could use Ambient capabilities, but at the time of this writing, Ambient capabilities are not supported on Kubernetes. This means that we can only use file capabilities or capability aware programs in order to get capabilities on programs running as nonroot on Kubernetes.\nConfiguring capabilities for our workloads At this point we know what are the differences with regards to capabilities when running our workloads with a root or a nonroot UID. In the next scenarios we are going to see how we can configure our workloads so they only get the required capabilities they need in order to run.\nWorkload running with root UID\nCreate a deployment for our workload:\nNOTE: We are dropping all of the runtime\u0026rsquo;s default capabilities, on top of that we add the NET_BIND_SERVICE capability and request the app to run with root UID. In the environment variables we configure our app to listen on port 80.\nNAMESPACE=test-capabilities cat \u0026lt;\u0026lt;EOF | kubectl -n ${NAMESPACE} create -f - apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: reversewords-app-rootuid name: reversewords-app-rootuid spec: replicas: 1 selector: matchLabels: app: reversewords-app-rootuid strategy: {} template: metadata: creationTimestamp: null labels: app: reversewords-app-rootuid spec: containers: - image: quay.io/mavazque/reversewords:ubi8 name: reversewords resources: {} env: - name: APP_PORT value: \u0026#34;80\u0026#34; securityContext: runAsUser: 0 capabilities: drop: - CHOWN - DAC_OVERRIDE - FSETID - FOWNER - SETGID - SETUID - SETPCAP - KILL add: - NET_BIND_SERVICE status: {} EOF We can check the logs for our application and see that it\u0026rsquo;s working fine:\nkubectl -n ${NAMESPACE} logs deployment/reversewords-app-rootuid 2021/04/01 09:59:39 Starting Reverse Api v0.0.18 Release: NotSet 2021/04/01 09:59:39 Listening on port 80 If we look at the capability sets this is what we get:\nkubectl -n ${NAMESPACE} exec -ti deployment/reversewords-app-rootuid -- grep Cap /proc/1/status CapInh:\t0000000000000400 CapPrm:\t0000000000000400 CapEff:\t0000000000000400 CapBnd:\t0000000000000400 CapAmb:\t0000000000000000 As expected, only NET_BIND_SERVICE capability is available:\ncapsh --decode=0000000000000400 0x0000000000000400=cap_net_bind_service The workload worked as expected when running with root UID, in the next scenario we will try the same app but this time running with a non-root UID.\nWorkload running with non-root UID\nCreate a deployment for our workload:\nNOTE: We are dropping all of the runtime\u0026rsquo;s default capabilities, on top of that we add the NET_BIND_SERVICE capability and request the app to run with non-root UID. In the environment variables we configure our app to listen on port 80.\nNAMESPACE=test-capabilities cat \u0026lt;\u0026lt;EOF | kubectl -n ${NAMESPACE} create -f - apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: reversewords-app-nonrootuid name: reversewords-app-nonrootuid spec: replicas: 1 selector: matchLabels: app: reversewords-app-nonrootuid strategy: {} template: metadata: creationTimestamp: null labels: app: reversewords-app-nonrootuid spec: containers: - image: quay.io/mavazque/reversewords:ubi8 name: reversewords resources: {} env: - name: APP_PORT value: \u0026#34;80\u0026#34; securityContext: runAsUser: 1024 capabilities: drop: - CHOWN - DAC_OVERRIDE - FSETID - FOWNER - SETGID - SETUID - SETPCAP - KILL add: - NET_BIND_SERVICE status: {} EOF We can check the logs for our application and see if it\u0026rsquo;s working:\nkubectl -n ${NAMESPACE} logs deployment/reversewords-app-nonrootuid 2021/04/01 10:09:10 Starting Reverse Api v0.0.18 Release: NotSet 2021/04/01 10:09:10 Listening on port 80 2021/04/01 10:09:10 listen tcp :80: bind: permission denied This time the application didn\u0026rsquo;t bind to port 80, let\u0026rsquo;s update the app configuration so it binds to port 8080 and then we will review the capability sets:\n# Patch the app so it binds to port 8080 kubectl -n ${NAMESPACE} patch deployment reversewords-app-nonrootuid -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;$setElementOrder/containers\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;reversewords\u0026#34;}],\u0026#34;containers\u0026#34;:[{\u0026#34;$setElementOrder/env\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;APP_PORT\u0026#34;}],\u0026#34;env\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;APP_PORT\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;8080\u0026#34;}],\u0026#34;name\u0026#34;:\u0026#34;reversewords\u0026#34;}]}}}}\u0026#39; # Get capability sets kubectl -n ${NAMESPACE} exec -ti deployment/reversewords-app-nonrootuid -- grep Cap /proc/1/status CapInh:\t0000000000000400 CapPrm:\t0000000000000000 CapEff:\t0000000000000000 CapBnd:\t0000000000000400 CapAmb:\t0000000000000000 We don\u0026rsquo;t have the NET_BIND_SERVICE in the effective set, if you remember from our previous post we would need the capability in the ambient set in order for our application to work, but as we said Kubernetes still doesn\u0026rsquo;t support ambient capabilities so our only option is make use of file capabilities.\nWe have created a new image for our application and our application binary now has the NET_BIND_SERVICE capability in the effective and permitted file capability sets. Let\u0026rsquo;s update the deployment configuration.\nNOTE: We configured the app to bind to port 80 and changed the container image with the one that has the required changes.\nkubectl -n ${NAMESPACE} patch deployment reversewords-app-nonrootuid -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;$setElementOrder/containers\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;reversewords\u0026#34;}],\u0026#34;containers\u0026#34;:[{\u0026#34;$setElementOrder/env\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;APP_PORT\u0026#34;}],\u0026#34;env\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;APP_PORT\u0026#34;,\u0026#34;value\u0026#34;:\u0026#34;80\u0026#34;}],\u0026#34;image\u0026#34;:\u0026#34;quay.io/mavazque/reversewords-captest:latest\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;reversewords\u0026#34;}]}}}}\u0026#39; We can check the logs for our application and see if it\u0026rsquo;s working:\nkubectl -n ${NAMESPACE} logs deployment/reversewords-app-nonrootuid 2021/04/01 10:18:42 Starting Reverse Api v0.0.21 Release: NotSet 2021/04/01 10:18:42 Listening on port 80 This time the application was able to bind to port 80, let\u0026rsquo;s review the capability sets:\nkubectl -n ${NAMESPACE} exec -ti deployment/reversewords-app-nonrootuid -- grep Cap /proc/1/status NOTE: Since our application binary has the required capability in its file capability sets the process thread was able to gain that capability:\nCapInh:\t0000000000000400 CapPrm:\t0000000000000400 CapEff:\t0000000000000400 CapBnd:\t0000000000000400 CapAmb:\t0000000000000000 We can check the file capability configured in our application binary:\nkubectl -n ${NAMESPACE} exec -ti deployment/reversewords-app-nonrootuid -- getcap /usr/bin/reverse-words /usr/bin/reverse-words = cap_net_bind_service+eip Seccomp Profiles on Kubernetes In this scenario we\u0026rsquo;re going to reuse the Secure Compute profile we created in the previous post.\nConfiguring Seccomp Profiles on the cluster nodes By default Kubelet will try to find the seccomp profiles in the /var/lib/kubelet/seccomp/ path. This path can be configured in the kubelet config.\nWe are going to create the two seccomp profiles that we will be using in the nodes.\nCreate below file on every node that can run workloads as /var/lib/kubelet/seccomp/centos8-ls.json:\nNOTE: This is the seccomp profile that allows us to run a centos8 image that runs ls / as we saw in the previous post.\n{ \u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ERRNO\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;SCMP_ARCH_X86_64\u0026#34; ], \u0026#34;syscalls\u0026#34;: [ { \u0026#34;names\u0026#34;: [ \u0026#34;access\u0026#34;, \u0026#34;arch_prctl\u0026#34;, \u0026#34;brk\u0026#34;, \u0026#34;capget\u0026#34;, \u0026#34;capset\u0026#34;, \u0026#34;chdir\u0026#34;, \u0026#34;close\u0026#34;, \u0026#34;epoll_ctl\u0026#34;, \u0026#34;epoll_pwait\u0026#34;, \u0026#34;execve\u0026#34;, \u0026#34;exit_group\u0026#34;, \u0026#34;fchown\u0026#34;, \u0026#34;fcntl\u0026#34;, \u0026#34;fstat\u0026#34;, \u0026#34;fstatfs\u0026#34;, \u0026#34;futex\u0026#34;, \u0026#34;getdents64\u0026#34;, \u0026#34;getpid\u0026#34;, \u0026#34;getppid\u0026#34;, \u0026#34;ioctl\u0026#34;, \u0026#34;mmap\u0026#34;, \u0026#34;mprotect\u0026#34;, \u0026#34;munmap\u0026#34;, \u0026#34;nanosleep\u0026#34;, \u0026#34;newfstatat\u0026#34;, \u0026#34;openat\u0026#34;, \u0026#34;prctl\u0026#34;, \u0026#34;pread64\u0026#34;, \u0026#34;prlimit64\u0026#34;, \u0026#34;read\u0026#34;, \u0026#34;rt_sigaction\u0026#34;, \u0026#34;rt_sigprocmask\u0026#34;, \u0026#34;rt_sigreturn\u0026#34;, \u0026#34;sched_yield\u0026#34;, \u0026#34;seccomp\u0026#34;, \u0026#34;set_robust_list\u0026#34;, \u0026#34;set_tid_address\u0026#34;, \u0026#34;setgid\u0026#34;, \u0026#34;setgroups\u0026#34;, \u0026#34;setuid\u0026#34;, \u0026#34;stat\u0026#34;, \u0026#34;statfs\u0026#34;, \u0026#34;tgkill\u0026#34;, \u0026#34;write\u0026#34; ], \u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;includes\u0026#34;: {}, \u0026#34;excludes\u0026#34;: {} } ] } Configuring seccomp profiles for our workloads Create a namespace:\nNAMESPACE=test-seccomp kubectl create ns ${NAMESPACE} Seccomp profiles can be configured at pod or container level, this time we\u0026rsquo;re going to configure it at pod level:\nNOTE: We configured the seccompProfile centos8-ls.json.\ncat \u0026lt;\u0026lt;EOF | kubectl -n ${NAMESPACE} create -f - apiVersion: v1 kind: Pod metadata: name: seccomp-ls-test spec: securityContext: seccompProfile: type: Localhost localhostProfile: centos8-ls.json containers: - image: registry.centos.org/centos:8 name: seccomp-ls-test command: [\u0026#34;ls\u0026#34;, \u0026#34;/\u0026#34;] dnsPolicy: ClusterFirst restartPolicy: Never status: {} EOF The pod was executed with no issues:\nkubectl -n ${NAMESPACE} logs seccomp-ls-test bin dev ... Let\u0026rsquo;s try to create a new pod that runs ls -l instead. On top of that we will configure the seccomp profile at the container level.\ncat \u0026lt;\u0026lt;EOF | kubectl -n ${NAMESPACE} create -f - apiVersion: v1 kind: Pod metadata: name: seccomp-lsl-test spec: containers: - image: registry.centos.org/centos:8 name: seccomp-lsl-test command: [\u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;, \u0026#34;/\u0026#34;] securityContext: seccompProfile: type: Localhost localhostProfile: centos8-ls.json dnsPolicy: ClusterFirst restartPolicy: Never status: {} EOF As expected, the pod failed since the seccomp profile doesn\u0026rsquo;t have all the required syscalls required for the command to run permitted:\nkubectl -n ${NAMESPACE} logs seccomp-lsl-test ls: cannot access \u0026#39;/\u0026#39;: Operation not permitted Closing Thoughts At this point you should\u0026rsquo;ve a clear understanding of when your workloads will benefit from using capabilities or seccomp profiles.\nWe\u0026rsquo;ve not been through how we can control which capabilities / seccomp a specific user can use, PodSecurityPolicies can be used to control such things on Kubernetes. In OpenShift you can use SecurityContextConstraints.\nIf you want to learn more around these topics feel free to take a look at the following SCCs lab: https://github.com/mvazquezc/scc-fun/blob/main/README.md\n","permalink":"https://linuxera.org/capabilities-seccomp-kubernetes/","summary":"Capabilities and Seccomp Profiles on Kubernetes In a previous post we talked about Linux Capabilities and Secure Compute Profiles, in this post we are going to see how we can leverage them on Kubernetes.\nWe will need a Kubernetes cluster, I\u0026rsquo;m going to use kcli in order to get one. Below command will deploy a Kubernetes cluster on VMs:\nNOTE: You can create a parameters file with the cluster configuration as well.","title":"Capabilities and Seccomp Profiles on Kubernetes"},{"content":"Container Security - Linux Capabilities and Secure Compute Profiles In this post we are going to see two security mechanisms used in Linux Containers in order to provide a security layer for our workloads.\nWe will see how Linux Capabilities and Secure Compute Profiles can be used for limiting the attack surface for our containers.\nThe first part of the blog post will be an introduction to Linux Capabilities and Secure Compute Profiles, second part will show how these technologies work through the use of demos.\nLinux Capabilities For the purpose of permission checks, traditional UNIX implementations distinguish two categories of processes:\nPrivileged Processes: Whose effective user ID is 0, referred to as superuser or root. Unprivileged Processes: Whose effective UID is nonzero. Privileged processes bypass all kernel permissions checks, on the other hand, unprivileged processes are subject to full permissions checking based on the processes credentials. Usually effective UID, effective GID and supplementary group list.\nStarting with kernel 2.2, Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled.\nYou need to keep in mind that capabilities are a per-thread attribute.\nThe first square represents root without capabilities before Linux kernel 2.2. The second square represents root with full capabilities. The third square represents root with only a few capabilities enabled. We can say that the power of root comes from the capabilities it can use rather than from being root itself. This will be important to understand that even if a container is running as root UID it doesn\u0026rsquo;t mean that it has full root privileges.\nAt the moment of this writing there are a total of 41 capabilities, you can find the list here. We are going to see some of the most common ones:\nCapability Allows NET_RAW Use RAW and PACKET sockets SETUID Make arbitrary manipulations of process UIDs CHOWN Marke arbitrary changes to file UIDs and GIDs SYS_PTRACE Trace arbitrary processes using ptrace SYS_TIME Set system clock Container runtimes have some of these capabilities enabled by default, for example, you can check the default capabilities enabled by the CRI-O runtime on its version v1.21 here.\nOne potential question you might have could be \u0026ldquo;What capabilities are required for my application?\u0026rdquo; - Well, knowing which capabilities are required by your applications requires a very good knowledge of the application by the developer. There is no magic tool that will tell you which capabilities are actually required.\nSecure Compute Profiles (Seccomp) Containers typically run a single application with a set of well-defined tasks, these applications usually require a small subset of the underlying operating system kernel APIs. For example, an httpd server does not require the mount syscall at all, why should the app have access to this syscall?\nIn order to limit the attack vector of a subverted process running in a container, the seccomp Linux kernel feature can be used to limit which syscalls a process has access to. We can think of seccomp like a firewall for syscalls.\nCreating your own seccomp profiles can be tedious and often requires deep knowledge of the application. For example, a developer must be aware that a framework that sets up a network server to accept connections would translate into calling socket, bind and listen system calls. This time, there is some tooling that can help us getting the list of syscalls used by our applications:\noci-seccomp-bpf-hook\nKeep in mind when using the oci hook for creating seccomp profiles for runtimes such as CRI-O that you need to run the hook with the proper container runtime, e.g: crun vs runc. strace\netc\nSecure Compute Profiles can be defined using JSON, below we will see an example:\n{ \u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ERRNO\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;SCMP_ARCH_X86_64\u0026#34;, \u0026#34;SCMP_ARCH_X86\u0026#34;, \u0026#34;SCMP_ARCH_X32\u0026#34; ], \u0026#34;syscalls\u0026#34;: [ { \u0026#34;names\u0026#34;: [ \u0026#34;accept4\u0026#34;, \u0026#34;epoll_wait\u0026#34;, \u0026#34;pselect6\u0026#34;, \u0026#34;futex\u0026#34; ], \u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34; } ] } Profiles can have multiple actions configured:\nAction Result SCMP_ACT_ALLOW Allows the use of the specified syscalls SCMP_ACT_ERRNO Denies the use of the specified syscalls SCMP_ACT_LOG Allows the use of any syscalls, but logs in the audit log the ones that are not explicitly permitted Above policy can be interpreted as:\nThe default action for syscalls not defined in the seccomp is deny, that means that we will only allow running syscalls explicitly permitted in our policy. The policy applies to the system architectures defined in architectures. We have a group of 4 syscalls that are permitted: accept4, epoll_wait, pselect6 and futex. Linux Capabilities in Action In the previous section on capabilities we said that capabilities are a per-thread attribute, and as such every thread has the following capability sets containing zero or more capabilities:\nPermitted Set Capabilities that the thread may assume. It also limits the capabilities that may be added to the inheritable set by a thread that has the SETPCAP capability in its effective set. If a thread drops a capability from its permitted set, it can never reacquire that capability unless it execve either a SETUID program or a program with that capability set as a permitted file capability. Inheritable Set Capabilities preserved across an execve. Inheritable capabilities remain inheritable when executing any program, and they will be added to the permitted set when executing a program that has that capability set as inheritable file capability. Keep in mind that inheritable capabilities are not generally preserved across execve when running as a non-root user, for such uses cases consider using ambient capabilities. Effective Set Capabilities used by the kernel to perform permission checks for the thread. Bounding Set Used to limit which capabilities can be gained during execve. Ambient Set Capabilities that are preserved across an execve of a program that is not privileged. No capability can ever be ambient if it\u0026rsquo;s not both permitted and inheritable. Executing a program that changes UID or GID due to SETUID or SETGID bits or executing a program that has file capabilities set will clear the ambient set. Ambient capabilities are added to the permitted set and assigned to the effective set when execve is called. On top of thread capabilities we have file capabilities, which are capabilities assigned to an executable file and that upon execution will be granted to the thread. These file capabilities are stored using one bit, but they act as different file capability sets:\nPermitted Set Capabilities that are automatically permitted to the thread, regardless of the thread\u0026rsquo;s inheritable capabilities. Inheritable Capabilities that are ANDed with the thread\u0026rsquo;s inheritable set to determine which inheritable capabilities are enabled in the permitted set of the thread after the execve. Effective This is not a capability set, but rather just a single bit. If set, during an execve all of the thread\u0026rsquo;s permitted capabilities are also raised in the effective set. If not set, after an execve, none of the thread\u0026rsquo;s permitted capabilities are raised in the effective set. Enabling a capability in the file effective set implies that the thread will acquire that capability in its permitted set. Capabilities and containers Before we get started with hands-on scenarios we need to know how capabilities behave in containers, specially what\u0026rsquo;s the different behaviours we get when running a container as root or as a non-root user.\nContainers running with UID 0\nWhen we run a container with UID 0, default capabilities configured by the runtime will be configured in the effective set for the container thread.\nPodman default runtime capabilities can be found here. You can also modify the defaults using the Podman\u0026rsquo;s configuration file.\nContainer running with nonroot UIDs\nWhen we run a container with a nonroot UID, default capabilities configured by the runtime are dropped, they will be in the inherited set and we can use file capabilities for such cases. We can also explicitly request a list of capabilities to the container runtime so those will be added to the container thread effective set.\nIn the next scenarios we will show the differences.\nGet capabilities assigned to a process During the following scenarios we will get capabilities assigned to processes, there are different ways of getting this information, let\u0026rsquo;s see some.\nLet\u0026rsquo;s run a test container, this container has an application that listens on a given port, but that\u0026rsquo;s not important for now:\npodman run -d --rm --name reversewords-test quay.io/mavazque/reversewords:latest We can always get capabilities for a process by querying the /proc filesystem:\n# Get container\u0026#39;s PID CONTAINER_PID=$(podman inspect reversewords-test --format \\{\\{.State.Pid\\}\\}) # Get caps for a given PID grep Cap /proc/${CONTAINER_PID}/status NOTE: The command returns the different capability sets in hex format, we will use a tool to decode that information.\nCapInh:\t00000000800405fb CapPrm:\t00000000800405fb CapEff:\t00000000800405fb CapBnd:\t00000000800405fb CapAmb:\t0000000000000000 We can see that the inherited, permitted, effective and bounding sets share the same capabilities, let\u0026rsquo;s decode them:\ncapsh --decode=00000000800405fb NOTE: As you can see below capabilities were assigned since those are the runtime\u0026rsquo;s defaults and our container is running with UID 0 so no capabilities were dropped.\n0x00000000800405fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_sys_chroot,cap_setfcap We can use podman inspect as well:\npodman inspect reversewords-test --format \\{\\{.EffectiveCaps\\}\\} [CAP_CHOWN CAP_DAC_OVERRIDE CAP_FOWNER CAP_FSETID CAP_KILL CAP_NET_BIND_SERVICE CAP_SETFCAP CAP_SETGID CAP_SETPCAP CAP_SETUID CAP_SYS_CHROOT] We can stop the test container now:\npodman stop reversewords-test Container running with UID 0 vs container running with nonroot UID We explained the different behaviour between a container running with root\u0026rsquo;s UID and with nonroot UID, now let\u0026rsquo;s see it in action.\nRun our test container with a root uid and get it\u0026rsquo;s capabilities:\n# Run the container podman run --rm -it --user 0 --entrypoint /bin/bash --name reversewords-test quay.io/mavazque/reversewords:ubi8 # Now we\u0026#39;re inside the container, let\u0026#39;s get caps grep Cap /proc/1/status CapInh:\t00000000800405fb CapPrm:\t00000000800405fb CapEff:\t00000000800405fb CapBnd:\t00000000800405fb CapAmb:\t0000000000000000 We can decode the capabilities in the effective set:\ncapsh --decode=00000000800405fb 0x00000000800405fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_sys_chroot,cap_setfcap We can exit our container now:\nexit Now it\u0026rsquo;s time to run our test container with a nonroot uid:\n# Run the container podman run --rm -it --user 1024 --entrypoint /bin/bash --name reversewords-test quay.io/mavazque/reversewords:ubi8 # Now we\u0026#39;re inside the container, let\u0026#39;s get caps grep Cap /proc/1/status NOTE: As you can see since we\u0026rsquo;re running with a nonroot UID our permitted and effective set were cleared. We could still use file capabilities.\nCapInh:\t00000000800405fb CapPrm:\t0000000000000000 CapEff:\t0000000000000000 CapBnd:\t00000000800405fb CapAmb:\t0000000000000000 We can exit our container now:\nexit On top of file capabilities, we can request specific capabilities to the runtime and they will be assigned to the corresponding sets even if we are running with a nonroot uid:\n# Run the container and request the NET_BIND_SERVICE capability podman run --rm -it --user 1024 --cap-add=cap_net_bind_service --entrypoint /bin/bash --name reversewords-test quay.io/mavazque/reversewords:ubi8 # Now we\u0026#39;re inside the container, let\u0026#39;s get caps grep Cap /proc/1/status NOTE: You can see that we got some capability in the permitted and effective set, let\u0026rsquo;s decode it.\nCapInh:\t00000000800405fb CapPrm:\t0000000000000400 CapEff:\t0000000000000400 CapBnd:\t00000000800405fb CapAmb:\t0000000000000400 Decode the capability:\ncapsh --decode=0000000000000400 NOTE: As expected, the NET_BIND_SERVICE capability was added to the containers permitted and effective set.\n0x0000000000000400=cap_net_bind_service We can exit our container now:\nexit Real world scenario We said that the power of root comes from its capabilities and not from just being root, in the next scenario we are going to show how we can use capabilities in order to run root-like actions with nonroot users.\nWe have our test application, it runs a small web-service on a given port. We want to bind to port 80, but as you might know, binding to ports under 1024 is a privileged action. Let\u0026rsquo;s see how capabilities can help us here.\nUsing thread capabilities\nWe can control in which port our application listens by using the APP_PORT environment variable. Let\u0026rsquo;s try to run our application in a non-privileged port with a non-privileged user:\npodman run --rm --user 1024 -e APP_PORT=8080 --name reversewords-test quay.io/mavazque/reversewords:ubi8 NOTE: As you can see the application is running properly.\n2021/03/27 17:12:49 Starting Reverse Api v0.0.18 Release: NotSet 2021/03/27 17:12:49 Listening on port 8080 You can stop the container by pressing Ctrl+C\nNow, let\u0026rsquo;s try to bind to port 80\npodman run --rm --user 1024 -e APP_PORT=80 --name reversewords-test quay.io/mavazque/reversewords:ubi8 NOTE: We got a permission denied, if you remember since we\u0026rsquo;re running with a nonroot UID the capability sets were cleared.\n2021/03/27 17:15:56 Starting Reverse Api v0.0.18 Release: NotSet 2021/03/27 17:15:56 Listening on port 80 2021/03/27 17:15:56 listen tcp :80: bind: permission denied We know that the capability NET_BIND_SERVICE allows unprivileged processes to bind to ports under 1024, let\u0026rsquo;s assign this capability to the container and see what happens:\npodman run --rm --user 1024 -e APP_PORT=80 --cap-add=cap_net_bind_service --name reversewords-test quay.io/mavazque/reversewords:ubi8 NOTE: Now the application was able to bind to port 80 even if it\u0026rsquo;s running with a nonroot user because the capability NET_BIND_SERVICE was added to the thread\u0026rsquo;s effective set.\n2021/03/27 17:18:07 Starting Reverse Api v0.0.18 Release: NotSet 2021/03/27 17:18:07 Listening on port 80 You can stop the container by pressing Ctrl+C\nUsing file capabilities\nFor this example we\u0026rsquo;re using the same application, but this time we set file capabilities to our application binary using the setcap command:\nNOTE: We added the NET_BIND_SERVICE in the effective and permitted file capability set.\nsetcap \u0026#39;cap_net_bind_service+ep\u0026#39; /usr/bin/reverse-words Let\u0026rsquo;s see what happens when we run this new image:\npodman run --rm -it --entrypoint /bin/bash --user 1024 -e APP_PORT=80 --name reversewords-test quay.io/mavazque/reversewords-captest:latest Instead of running the application directly, we opened a shell. Let\u0026rsquo;s review the file capabilities assigned to our binary:\ngetcap /usr/bin/reverse-words NOTE: As previously mentioned, NET_BIND_SERVICE capability was added.\n/usr/bin/reverse-words = cap_net_bind_service+ep Let\u0026rsquo;s see the container thread capabilities:\ngrep Cap /proc/1/status NOTE: We don\u0026rsquo;t have the NET_BIND_SERVICE capability in the effective set, which means that we won\u0026rsquo;t be able to bind to port 80 under normal circumstances. If we decode the inherited set we will see that the NET_BIND_SERVICE capability is present, that means that we should be able to use file capabilities to get that capability in the effective and permitted set.\nCapInh:\t00000000800405fb CapPrm:\t0000000000000000 CapEff:\t0000000000000000 CapBnd:\t00000000800405fb CapAmb:\t0000000000000000 Let\u0026rsquo;s try to run our application:\n/usr/bin/reverse-words NOTE: We were able to bind to port 80 since the file capability granted access to NET_BIND_SERVICE to our application thread.\n2021/03/27 17:26:51 Starting Reverse Api v0.0.18 Release: NotSet 2021/03/27 17:26:51 Listening on port 80 We can exit our container now:\nexit You might be thinking that file capabilities can be used to bypass the thread\u0026rsquo;s capabilities, but that\u0026rsquo;s not the case. Let\u0026rsquo;s see what happens when we try to get a capability via file capabilities when the capability we want to get is not in the thread\u0026rsquo;s inherited set:\n# We explicitly request to drop all capabilities podman run --rm -it --entrypoint /bin/bash --user 1024 --cap-drop=all -e APP_PORT=80 --name reversewords-test quay.io/mavazque/reversewords-captest:latest Let\u0026rsquo;s see the container thread capabilities:\ngrep Cap /proc/1/status NOTE: We don\u0026rsquo;t have any capability in any capability set for the thread.\nCapInh:\t0000000000000000 CapPrm:\t0000000000000000 CapEff:\t0000000000000000 CapBnd:\t0000000000000000 CapAmb:\t0000000000000000 If we try to run our application:\nNOTE: The kernel stopped us from getting the NET_BIND_SERVICE and thus executing our app.\nbash: /usr/bin/reverse-words: Operation not permitted We can exit our container now:\nexit Capability aware programs Very similar to file capabilities there are programs that are capability aware, that happens when they use specific libraries that are used for managing capabilities at a thread level.\nIn the previous example, our application raised the NET_BIND_SERVICE capability in the effective set for the whole execution time. Capability aware programs are much smarter and they only raise capabilities when they\u0026rsquo;re required and they drop those capabilities when they\u0026rsquo;re no longer required.\nIf our application was that smarter it would\u0026rsquo;ve raised the NET_BIND_SERVICE before binding to port 80, and once binded it would\u0026rsquo;ve dropped the capability since it was not required anymore.\nFor example, we can build capability aware programs in go by using a library like this.\nSecure Compute Profiles in Action In this scenario we will generate a seccomp profile for our container, in order to do that we will use the OCI Hook project.\nNOTE: The OCI Hook requires us to run containers with a privileged user, that\u0026rsquo;s why we will be using sudo in the next commands.\nRun a container that runs ls / command and tell the hook to save the seccomp profile at /tmp/ls.json:\nsudo podman run --rm --annotation io.containers.trace-syscall=\u0026#34;of:/tmp/ls.json\u0026#34; fedora:32 ls / \u0026gt; /dev/null The hook generated the seccomp profile at /tmp/ls.json, let\u0026rsquo;s review it:\ncat /tmp/ls.json | jq NOTE: We can see the syscalls that were made by our container in order to run the ls / command.\n{ \u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ERRNO\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;SCMP_ARCH_X86_64\u0026#34; ], \u0026#34;syscalls\u0026#34;: [ { \u0026#34;names\u0026#34;: [ \u0026#34;access\u0026#34;, \u0026#34;arch_prctl\u0026#34;, \u0026#34;brk\u0026#34;, \u0026#34;capset\u0026#34;, \u0026#34;close\u0026#34;, \u0026#34;execve\u0026#34;, \u0026#34;exit_group\u0026#34;, \u0026#34;fstat\u0026#34;, \u0026#34;getdents64\u0026#34;, \u0026#34;ioctl\u0026#34;, \u0026#34;mmap\u0026#34;, \u0026#34;mprotect\u0026#34;, \u0026#34;munmap\u0026#34;, \u0026#34;openat\u0026#34;, \u0026#34;prctl\u0026#34;, \u0026#34;pread64\u0026#34;, \u0026#34;prlimit64\u0026#34;, \u0026#34;read\u0026#34;, \u0026#34;rt_sigaction\u0026#34;, \u0026#34;rt_sigprocmask\u0026#34;, \u0026#34;select\u0026#34;, \u0026#34;set_robust_list\u0026#34;, \u0026#34;set_tid_address\u0026#34;, \u0026#34;setresgid\u0026#34;, \u0026#34;setresuid\u0026#34;, \u0026#34;stat\u0026#34;, \u0026#34;statfs\u0026#34;, \u0026#34;write\u0026#34; ], \u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;includes\u0026#34;: {}, \u0026#34;excludes\u0026#34;: {} } ] } Now that we have a seccomp profile that only has the required syscalls for our application to work, we can run the container with this profile:\npodman run --rm --security-opt seccomp=/tmp/ls.json fedora:32 ls / \u0026gt; /dev/null It worked!, let\u0026rsquo;s see what happens if we change the ls command a bit:\npodman run --rm --security-opt seccomp=/tmp/ls.json fedora:32 ls -l / \u0026gt; /dev/null NOTE: The ls -l command failed because it requires additional syscalls that are not permitted by our seccomp profile.\nls: cannot access \u0026#39;/\u0026#39;: Operation not permitted The hook allow us to pass an input file that will be used as baseline, then we will log the required additional syscalls into a new output file:\nsudo podman run --rm --annotation io.containers.trace-syscall=\u0026#34;if:/tmp/ls.json;of:/tmp/lsl.json\u0026#34; fedora:32 ls -l / \u0026gt; /dev/null An updated seccomp profile has been generated at /tmp/lsl.json, let\u0026rsquo;s compare both profiles:\ndiff \u0026lt;(jq -S . /tmp/ls.json) \u0026lt;(jq -S . /tmp/lsl.json) NOTE: We can see the additional syscalls required by the ls -l command below.\n42a43,61 \u0026gt; }, \u0026gt; { \u0026gt; \u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;, \u0026gt; \u0026#34;args\u0026#34;: [], \u0026gt; \u0026#34;comment\u0026#34;: \u0026#34;\u0026#34;, \u0026gt; \u0026#34;excludes\u0026#34;: {}, \u0026gt; \u0026#34;includes\u0026#34;: {}, \u0026gt; \u0026#34;names\u0026#34;: [ \u0026gt; \u0026#34;connect\u0026#34;, \u0026gt; \u0026#34;fcntl\u0026#34;, \u0026gt; \u0026#34;futex\u0026#34;, \u0026gt; \u0026#34;getpid\u0026#34;, \u0026gt; \u0026#34;getxattr\u0026#34;, \u0026gt; \u0026#34;lgetxattr\u0026#34;, \u0026gt; \u0026#34;lseek\u0026#34;, \u0026gt; \u0026#34;lstat\u0026#34;, \u0026gt; \u0026#34;readlink\u0026#34;, \u0026gt; \u0026#34;socket\u0026#34; \u0026gt; ] If we try to use the new seccomp profile we will be able to run the ls -l command this time:\npodman run --rm --security-opt seccomp=/tmp/lsl.json fedora:32 ls -l / \u0026gt; /dev/null Closing Thoughts In this blog post we introduced two security technologies in containers that can be used to limit the attack surface in our applications running in containers. In a future blog post we will see how these technologies can be leveraged in Kubernetes.\nSources Linux Capabilities in OpenShift Linux Man Pages ","permalink":"https://linuxera.org/container-security-capabilities-seccomp/","summary":"Container Security - Linux Capabilities and Secure Compute Profiles In this post we are going to see two security mechanisms used in Linux Containers in order to provide a security layer for our workloads.\nWe will see how Linux Capabilities and Secure Compute Profiles can be used for limiting the attack surface for our containers.\nThe first part of the blog post will be an introduction to Linux Capabilities and Secure Compute Profiles, second part will show how these technologies work through the use of demos.","title":"Container Security - Linux Capabilities and Secure Compute Profiles"},{"content":"Containers are Linux You probably already heard this expression, in today\u0026rsquo;s post we are going to desmitify container technologies by decomposing them part by part and describing which Linux technologies make containers possible.\nWe can describe a container as an isolated process running on a host. In order to isolate the process the container runtimes leverage Linux kernel technologies such as: namespaces, chroots, cgroups, etc. plus security layers like SELinux.\nWe will see how we can leverage these technologies on Linux in order to build and run our own containers.\nContainer File Systems (a.k.a rootfs) Whenever you pull an image container from a container registry, you\u0026rsquo;re downloading just a tarball. We can say container images are just tarballs.\nThere are multiple ways to get a rootfs that we can use in order to run our containers, for this blogpost we\u0026rsquo;re going to download an already built rootfs for Alpine Linux.\nThere are tools such as buildroot that make it really easy to create our own rootfs. We will see how to create our own rootfs using buildroot on a future post.\nAs earlier mentioned, let\u0026rsquo;s download the x86_64 rootfs for Alpine Linux:\nmkdir /var/tmp/alpine-rootfs/ \u0026amp;\u0026amp; cd $_ curl https://dl-cdn.alpinelinux.org/alpine/v3.12/releases/x86_64/alpine-minirootfs-3.12.3-x86_64.tar.gz -o rootfs.tar.gz We can extract the rootfs on the temporary folder we just created:\ntar xfz rootfs.tar.gz \u0026amp;\u0026amp; rm -f rootfs.tar.gz If we take a look at the extracted files:\ntree -L 1 As you can see, the result looks like a Linux system. We have some well known directories in the Linux Filesystem Hierarchy Standard such as: bin, tmp, dev, opt, etc.\n. ├── bin ├── dev ├── etc ├── home ├── lib ├── media ├── mnt ├── opt ├── proc ├── root ├── run ├── sbin ├── srv ├── sys ├── tmp ├── usr └── var chroot Chroot is an operation that changes the root directory for the current running process and their children. A process that runs inside a chroot cannot access files and commands outside the chrooted directory tree.\nThat being said, we can now chroot into the rootfs environment we extracted in the previous step and run a shell to poke around:\nCreate the chroot jail\nsudo chroot /var/tmp/alpine-rootfs/ /bin/sh Check the os-release\ncat /etc/os-release NAME=\u0026#34;Alpine Linux\u0026#34; ID=alpine VERSION_ID=3.12.3 PRETTY_NAME=\u0026#34;Alpine Linux v3.12\u0026#34; HOME_URL=\u0026#34;https://alpinelinux.org/\u0026#34; BUG_REPORT_URL=\u0026#34;https://bugs.alpinelinux.org/\u0026#34; Try to list /tmp/alpine-rootfs folder\nls /var/tmp/alpine-rootfs ls: /var/tmp/alpine-rootfs: No such file or directory As you can see we only have visibility of the contents of the rootfs we\u0026rsquo;ve chroot into.\nWe can now install python and run a simple http server for example:\nInstall python3\napk add python3 Run a simple http server\nNOTE: When we execute the Python interpreter we\u0026rsquo;re actually running it from /var/tmp/alpine-rootfs/usr/bin/python3\npython3 -m http.server If you open a new shell on your system (even if it\u0026rsquo;s outside of the chroot) you will be able to reach the http server we just created:\ncurl http://127.0.0.1:8000 namespaces At this point we were able to work with a tarball like if it was a different system, but we\u0026rsquo;re not isolating the processed from the host system like containers do.\nLet\u0026rsquo;s check the level of isolation:\nIn a shell outside the chroot run a ping command:\nping 127.0.0.1 Mount the proc filesystem inside the chrooted shell\nNOTE: If you\u0026rsquo;re still running the python http server you can kill the process\nmount -t proc proc /proc Run a ps command inside the chroot and try to find the ping command:\nps -ef | grep \u0026#34;ping 127.0.0.1\u0026#34; 387870 1000 0:00 ping 127.0.0.1 388204 root 0:00 grep ping 127.0.0.1 We have visibility over the host system processes, that\u0026rsquo;s not great. On top of that, our chroot is running as root so we can even kill the process:\npkill -f \u0026#34;ping 127.0.0.1\u0026#34; Now is when we introduce namespaces.\nLinux namespaces are a feature of the Linux kernel that partitions kernel resources so one process will only see a set of resources while a different process can see a different set of resources.\nThese resources may exist in multiple spaces. The list of existing namespaces are:\nNamespace Isolates Cgroup Cgroup root directory IPC System V IPC, POSIX message queues Network Network devices, stacks, prots, etc. Mount Mount points PID Process IDs Time Boot and monotonic clocks User User and Group IDs UTS Hostname and NIS domain name Creating namespaces with unshare Creating namespaces is just a single syscall (unshare). There is also a unshare command line tool that provides a nice wrapper around the syscall.\nWe are going to use the unshare command line to create namespaces manually. Below example will create a PID namespace for the chrooted shell:\nExit the chroot we have already running\nNOTE: Run below command on the chrooted shell\nexit Create the PID namespace and run the chrooted shell inside the namespace\nsudo unshare -p -f --mount-proc=/var/tmp/alpine-rootfs/proc chroot /var/tmp/alpine-rootfs/ /bin/sh Now that we have created our new process namespace, we will see that our shell thinks its PID is 1:\nps -ef NOTE: As you can see, we no longer see the host system processes\nPID USER TIME COMMAND 1 root 0:00 /bin/sh 2 root 0:00 ps -ef Since we didn\u0026rsquo;t create a namespace for the network we can still see the whole network stack from the host system:\nip -o a NOTE: Below output might vary on your system\n1: lo inet 127.0.0.1/8 scope host lo\\ valid_lft forever preferred_lft forever 1: lo inet6 ::1/128 scope host \\ valid_lft forever preferred_lft forever 4: wlp82s0 inet 192.168.0.160/24 brd 192.168.0.255 scope global dynamic wlp82s0\\ valid_lft 6555sec preferred_lft 6555sec 4: wlp82s0 inet6 fe80::4e03:6176:40f0:b862/64 scope link \\ valid_lft forever preferred_lft forever Entering namespaces with nsenter One powerful thing about namespaces is that they\u0026rsquo;re pretty flexible, for example you can have processes with some separated namespaces and some shared namespaces. One example in the Kubernetes world will be containers running in pods: Containers will have different PID namespaces but they will share the Network namespace.\nThere is a syscall (setns) that can be used to reassociate a thread with a namespace. The nsenter command line tool will help with that.\nWe can check the namespaces for a given process by querying the /proc filesystem:\nFrom a shell outside the chroot get the PID for the chrooted shell\nUNSHARE_PPID=$(ps -ef | grep \u0026#34;sudo unshare\u0026#34; | grep chroot | awk \u0026#39;{print $2}\u0026#39;) UNSHARE_PID=$(ps -ef | grep ${UNSHARE_PPID} | grep chroot | grep -v sudo | awk \u0026#39;{print $2}\u0026#39;) SHELL_PID=$(ps -ef | grep ${UNSHARE_PID} | grep -v chroot | grep /bin/sh | awk \u0026#39;{print $2}\u0026#39;) ps -ef | grep ${UNSHARE_PID} | grep -v chroot | grep /bin/sh root 390072 390071 0 12:32 pts/1 00:00:00 /bin/sh From a shell outside the chroot get the namespaces for the shell process:\nsudo ls -l /proc/${SHELL_PID}/ns total 0 lrwxrwxrwx. 1 root root 0 mar 25 12:54 cgroup -\u0026gt; \u0026#39;cgroup:[4026531835]\u0026#39; lrwxrwxrwx. 1 root root 0 mar 25 12:54 ipc -\u0026gt; \u0026#39;ipc:[4026531839]\u0026#39; lrwxrwxrwx. 1 root root 0 mar 25 12:54 mnt -\u0026gt; \u0026#39;mnt:[4026532266]\u0026#39; lrwxrwxrwx. 1 root root 0 mar 25 12:54 net -\u0026gt; \u0026#39;net:[4026532008]\u0026#39; lrwxrwxrwx. 1 root root 0 mar 25 12:54 pid -\u0026gt; \u0026#39;pid:[4026532489]\u0026#39; lrwxrwxrwx. 1 root root 0 mar 25 12:54 pid_for_children -\u0026gt; \u0026#39;pid:[4026532489]\u0026#39; lrwxrwxrwx. 1 root root 0 mar 25 12:54 time -\u0026gt; \u0026#39;time:[4026531834]\u0026#39; lrwxrwxrwx. 1 root root 0 mar 25 12:54 time_for_children -\u0026gt; \u0026#39;time:[4026531834]\u0026#39; lrwxrwxrwx. 1 root root 0 mar 25 12:54 user -\u0026gt; \u0026#39;user:[4026531837]\u0026#39; lrwxrwxrwx. 1 root root 0 mar 25 12:54 uts -\u0026gt; \u0026#39;uts:[4026531838]\u0026#39; Earlier we saw how we were just setting a different PID namespace, let\u0026rsquo;s see the difference between the PID namespace configured for our chroot shell and for the regular shell:\nNOTE: Below commands must be run from a shell outside the chroot:\nGet PID namespace for the chrooted shell:\nsudo ls -l /proc/${SHELL_PID}/ns/pid lrwxrwxrwx. 1 root root 0 mar 25 12:54 pid -\u0026gt; pid:[4026532489] Get PID namespace for the regular shell:\nsudo ls -l /proc/$$/ns/pid lrwxrwxrwx. 1 mario mario 0 mar 25 12:55 pid -\u0026gt; pid:[4026531836] As you can see, both processes are using a different PID namespace. We saw that network stack was still visible, let\u0026rsquo;s see if there is any difference in the Network namespace for both processes. Let\u0026rsquo;s start with the chrooted shell:\nsudo ls -l /proc/${SHELL_PID}/ns/net lrwxrwxrwx. 1 root root 0 mar 25 12:54 net -\u0026gt; net:[4026532008] Now, time to get the one for the regular shell:\nsudo ls -l /proc/$$/ns/net lrwxrwxrwx. 1 mario mario 0 mar 25 12:55 net -\u0026gt; net:[4026532008] As you can see from above outputs, both processes are using the same Network namespace.\nIf we want to join a process to an existing namespace we can do that using nsenter as we said before, let\u0026rsquo;s do that.\nOpen a new shell outside the chroot\nWe want run a new chrooted shell and join the already existing PID namespace we created earlier:\n# Get the previous unshare PPID UNSHARE_PPID=$(ps -ef | grep \u0026#34;sudo unshare\u0026#34; | grep chroot | awk \u0026#39;{print $2}\u0026#39;) # Get the previous unshare PID UNSHARE_PID=$(ps -ef | grep ${UNSHARE_PPID} | grep chroot | grep -v sudo | awk \u0026#39;{print $2}\u0026#39;) # Get the previous chrooted shell PID SHELL_PID=$(ps -ef | grep ${UNSHARE_PID} | grep -v chroot | grep /bin/sh | awk \u0026#39;{print $2}\u0026#39;) # We will enter the previous PID namespace, remount the /proc filesystem and run a new chrooted shell sudo nsenter --pid=/proc/${SHELL_PID}/ns/pid unshare -f --mount-proc=/tmp/alpine-rootfs/proc chroot /tmp/alpine-rootfs/ /bin/sh From the new chrooted shell we can run a ps command and we should see the existing processes from the previous chrooted shell:\nps -ef PID USER TIME COMMAND 1 root 0:00 /bin/sh 4 root 0:00 unshare -f --mount-proc=/tmp/alpine-rootfs/proc chroot /tmp/alpine-rootfs/ /bin/sh 5 root 0:00 /bin/sh 6 root 0:00 ps -ef We have entered the already existing PID namespace used by our previous chrooted shell and we can see that running a ps command from the new shell (PID 5) we can see the first shell (PID 1).\nInjecting files or directories into the chroot Containers are usually inmutables, that means that we cannot create or edit directories or files into the chroot. Sometimes we will need to inject files or directories either for storage or configuration. We are going to show how we can create some files on the host system and expose them as read-only to the chrooted shell using mount.\nCreate a folder in the host system to host some read-only config files:\nsudo mkdir -p /var/tmp/alpine-container-configs/ echo \u0026#34;Test\u0026#34; | sudo tee -a /var/tmp/alpine-container-configs/app-config echo \u0026#34;Test2\u0026#34; | sudo tee -a /var/tmp/alpine-container-configs/srv-config Create a folder in the rootfs directory to use it as mount point:\nsudo mkdir -p /var/tmp/alpine-rootfs/etc/myconfigs Run a bind mount:\nsudo mount --bind -o ro /var/tmp/alpine-container-configs /var/tmp/alpine-rootfs/etc/myconfigs Run a chrooted shell and check the mounted files:\nNOTE: You can exit from the already existing chrooted shells before creating this one\nsudo unshare -p -f --mount-proc=/var/tmp/alpine-rootfs/proc chroot /var/tmp/alpine-rootfs/ /bin/sh ls -l /etc/myconfigs/ total 8 -rw-r--r-- 1 root root 5 Mar 25 13:28 app-config -rw-r--r-- 1 root root 6 Mar 25 13:28 srv-config If we try to edit the files from the chrooted shell, this is what happens:\necho \u0026#34;test3\u0026#34; \u0026gt;\u0026gt; /etc/myconfigs/app-config NOTE: We cannot edit/create files since the mount is read-only\n/bin/sh: can\u0026#39;t create /etc/myconfigs/app-config: Read-only file system If we want to unmount the files we can run the command below from the host system:\nsudo umount /var/tmp/alpine-rootfs/etc/myconfigs CGroups Control groups allow the kernel to restrict resources like memory and CPU for specific processes. In this case we are going to create a new CGroup for our chrooted shell so it cannot use more than 200MB of RAM.\nKernel exposes cgroups at the /sys/fs/cgroup directory:\nls /sys/fs/cgroup/ cgroup.controllers cgroup.stat cpuset.cpus.effective io.cost.model machine.slice system.slice cgroup.max.depth cgroup.subtree_control cpuset.mems.effective io.cost.qos memory.numa_stat user.slice cgroup.max.descendants cgroup.threads cpu.stat io.pressure memory.pressure cgroup.procs cpu.pressure init.scope io.stat memory.stat Let\u0026rsquo;s create a new cgroup, we just need to create a folder for that to happen:\nsudo mkdir /sys/fs/cgroup/alpinecgroup ls /sys/fs/cgroup/alpinecgroup/ NOTE: The kernel automatically populated the folder\ncgroup.controllers cgroup.stat io.pressure memory.max memory.swap.current pids.max cgroup.events cgroup.subtree_control memory.current memory.min memory.swap.events cgroup.freeze cgroup.threads memory.events memory.numa_stat memory.swap.high cgroup.max.depth cgroup.type memory.events.local memory.oom.group memory.swap.max cgroup.max.descendants cpu.pressure memory.high memory.pressure pids.current cgroup.procs cpu.stat memory.low memory.stat pids.events Now, we just need to adjust the memory value by modifying the required files:\n# Set a limit of 200MB of RAM echo \u0026#34;200000000\u0026#34; | sudo tee -a /sys/fs/cgroup/alpinecgroup/memory.max # Disable swap echo \u0026#34;0\u0026#34; | sudo tee -a /sys/fs/cgroup/alpinecgroup/memory.swap.max Finally, we need to assign this CGroup to our chrooted shell:\n# Get the previous unshare PPID UNSHARE_PPID=$(ps -ef | grep \u0026#34;sudo unshare\u0026#34; | grep chroot | awk \u0026#39;{print $2}\u0026#39;) # Get the previous unshare PID UNSHARE_PID=$(ps -ef | grep ${UNSHARE_PPID} | grep chroot | grep -v sudo | awk \u0026#39;{print $2}\u0026#39;) # Get the previous chrooted shell PID SHELL_PID=$(ps -ef | grep ${UNSHARE_PID} | grep -v chroot | grep /bin/sh | awk \u0026#39;{print $2}\u0026#39;) # Assign the shell process to the cgroup echo ${SHELL_PID} | sudo tee -a /sys/fs/cgroup/alpinecgroup/cgroup.procs In order to test the cgroup we will create a dumb python script in the chrooted shell:\n# Mount the /dev fs since we need to read data from urandom mount -t devtmpfs dev /dev # Create the python script cat \u0026lt;\u0026lt;EOF \u0026gt; /opt/dumb.py f = open(\u0026#34;/dev/urandom\u0026#34;, \u0026#34;r\u0026#34;, encoding = \u0026#34;ISO-8859-1\u0026#34;) data = \u0026#34;\u0026#34; i=0 while i \u0026lt; 20: data += f.read(10000000) # 10mb i += 1 print(\u0026#34;Used %d MB\u0026#34; % (i * 10)) EOF Run the script:\npython3 /opt/dumb.py NOTE: The process was killed before it reached the memory limit.\npython3 /opt/dumb.py Used 10 MB Used 20 MB Used 30 MB Used 40 MB Used 50 MB Used 60 MB Used 70 MB Used 80 MB Used 90 MB Used 100 MB Used 110 MB Used 120 MB Used 130 MB Used 140 MB Used 150 MB Used 160 MB Used 170 MB Killed We can now close the chrooted shell and remove the cgroup\nExit the chrooted shell:\nexit NOTE: A CGroup cannot be removed until all the attached processes are finished.\nRemove the cgroup:\nsudo rmdir /sys/fs/cgroup/alpinecgroup/ Container security and capabilities As you know, Linux containers run directly on top of the host system and share multiple resources like the Kernel, filesystems, network stack, etc. If an attacker breaks out of the container confinement security risks will arise.\nIn order to limit the attack surface there are many technologies involved in limiting the power of processes running in the container such as SELinux, Security Compute Profiles and Linux Capabilities.\nYou can learn more in this blogpost.\nClosing Thoughts Containers are not new, they use technologies that have been present in the Linux kernel for a long time. Tools like Podman or Docker make running containers easy for everyone by abstracting the different Linux technologies used under the hood from the user.\nI hope that now you have a better understanding of what technologies are used when you run containers on your systems.\nSources Containers from from Scratch Creating CGroupsv2 ","permalink":"https://linuxera.org/containers-under-the-hood/","summary":"Containers are Linux You probably already heard this expression, in today\u0026rsquo;s post we are going to desmitify container technologies by decomposing them part by part and describing which Linux technologies make containers possible.\nWe can describe a container as an isolated process running on a host. In order to isolate the process the container runtimes leverage Linux kernel technologies such as: namespaces, chroots, cgroups, etc. plus security layers like SELinux.","title":"Containers under the Hood"},{"content":"Introduction This post is a continuation of our previous blog Writing Operators using the Operator Framework SDK.\nWe will continue working on the operator created on the previous blog, if you want to be able to follow this blog, you will need to run the steps from the previous blog.\nOperator Lifecycle Manager The Operator Lifecycle Manager is an open source toolkit to manage Operators in an effective, automated and scalable way. You can learn more here.\nDuring this post we will integrate our Reverse Words Operator into OLM, that way we will be able to use OLM to manager our Operator lifecycle.\nIntegrating Reverse Words Operator into OLM Deploy OLM Some Kubernetes distributions, like OpenShift come with OLM pre-installed, if you\u0026rsquo;re using a distribution with doesn\u0026rsquo;t come with OLM or you don\u0026rsquo;t know if your Kubernetes cluster is running OLM already you can use the operator-sdk command to find out.\noperator-sdk olm status If your cluster is not running olm and you want to deploy it, you can run the following command:\noperator-sdk olm install Once we know OLM is present in our cluster we can continue and start the creation of our Operator Bundle.\nOperator Bundle An Operator Bundle consists of different manifests (CSVs and CRDs) and some metadata that defines the Operator at a specific version.\nYou can read more about Bundles here.\nCreating the Operator Bundle We need to change to the Reverse Words Operator folder and run the make bundle command. We will be asked for some information.\ncd ~/operators-projects/reverse-words-operator/ QUAY_USERNAME=\u0026lt;username\u0026gt; make bundle VERSION=0.0.1 CHANNELS=alpha DEFAULT_CHANNEL=alpha IMG=quay.io/$QUAY_USERNAME/reversewords-operator:v0.0.1 NOTE: Example output\nDisplay name for the operator (required): \u0026gt; Reverse Words Operator Description for the operator (required): \u0026gt; Deploys and Manages instances of the Reverse Words Application Provider\u0026#39;s name for the operator (required): \u0026gt; Linuxera Any relevant URL for the provider name (optional): \u0026gt; linuxera.org Comma-separated list of keywords for your operator (required): \u0026gt; reverse,reversewords,linuxera Comma-separated list of maintainers and their emails (e.g. \u0026#39;name1:email1, name2:email2\u0026#39;) (required): \u0026gt; mario@linuxera.org \u0026lt;omitted_output\u0026gt; Above command has generated some files:\nbundle ├── manifests │ ├── apps.linuxera.org_reversewordsapps.yaml │ ├── reverse-words-operator.clusterserviceversion.yaml │ ├── reverse-words-operator-controller-manager-metrics-service_v1_service.yaml │ ├── reverse-words-operator-manager-config_v1_configmap.yaml │ └── reverse-words-operator-metrics-reader_rbac.authorization.k8s.io_v1beta1_clusterrole.yaml ├── metadata │ └── annotations.yaml └── tests └── scorecard └── config.yaml We need to tweak the ClusterServiceVersion a bit:\nConfigure proper installModes Add WATCH_NAMESPACE env var to the operator deployment Add an Icon to our Operator You can download the modified CSV here:\ncurl -Ls https://linuxera.org/integrating-operators-olm/reverse-words-operator.clusterserviceversion_v0.0.1.yaml -o ~/operators-projects/reverse-words-operator/bundle/manifests/reverse-words-operator.clusterserviceversion.yaml sed -i \u0026#34;s/QUAY_USER/$QUAY_USERNAME/g\u0026#34; ~/operators-projects/reverse-words-operator/bundle/manifests/reverse-words-operator.clusterserviceversion.yaml Now that we have the Operator Bundle ready we can build it and push it to Quay. After that we will build the index image and once the index image is ready, we will use it to deploy our operator.\nNOTE: I\u0026rsquo;ll be using podman, you can use docker as well\nBuild the bundle\npodman build -f bundle.Dockerfile -t quay.io/$QUAY_USERNAME/reversewords-operator-bundle:v0.0.1 Push and validate the bundle\npodman push quay.io/$QUAY_USERNAME/reversewords-operator-bundle:v0.0.1 operator-sdk bundle validate quay.io/$QUAY_USERNAME/reversewords-operator-bundle:v0.0.1 -b podman Create the Index Image\n# Download opm tool sudo curl -sL https://github.com/operator-framework/operator-registry/releases/download/v1.13.8/linux-amd64-opm -o /usr/local/bin/opm \u0026amp;\u0026amp; chmod +x /usr/local/bin/opm # Create the index image opm index add -c podman --bundles quay.io/$QUAY_USERNAME/reversewords-operator-bundle:v0.0.1 --tag quay.io/$QUAY_USERNAME/reversewords-index:v0.0.1 # Push the index image podman push quay.io/$QUAY_USERNAME/reversewords-index:v0.0.1 Deploy the Operator using OLM At this point we have our bundle and index image ready, we just need to create the required CatalogSource into the cluster so we get access to our Operator bundle.\nOLM_NAMESPACE=$(kubectl get pods -A | grep catalog-operator | awk \u0026#39;{print $1}\u0026#39;) cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: reversewords-catalog namespace: $OLM_NAMESPACE spec: sourceType: grpc image: quay.io/$QUAY_USERNAME/reversewords-index:v0.0.1 EOF A pod will be created on the OLM namespace:\nkubectl -n $OLM_NAMESPACE get pod -l olm.catalogSource=reversewords-catalog NAME READY STATUS RESTARTS AGE reversewords-catalog-jdn78 1/1 Running 0 3m11s OLM will read the CSVs from our Operator Bundle and will load the Package Manifest into the cluster:\nkubectl get packagemanifest -l catalog=reversewords-catalog NAME CATALOG AGE reverse-words-operator 4m9s At this point we can create a Subscription to our operator:\nCreate a new namespace\nNAMESPACE=test-operator-subscription kubectl create ns $NAMESPACE Create the subscription in the namespace we just created\ncat \u0026lt;\u0026lt;EOF | kubectl -n $NAMESPACE create -f - apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: reversewords-subscription spec: channel: alpha name: reverse-words-operator installPlanApproval: Automatic source: reversewords-catalog sourceNamespace: $OLM_NAMESPACE --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: reverse-words-operatorgroup spec: targetNamespaces: - $NAMESPACE EOF The operator will be deployed in the namespace\nkubectl -n $NAMESPACE get pods NAME READY STATUS RESTARTS AGE reverse-words-operator-controller-manager-7c57649d7f-x88w5 2/2 Running 0 5m5s Publish an upgrade for our Operator We have seen how to create a bundle for our operator, now we are going to see how we can add new versions to the bundle and link them so we can publish updates for the operators.\nIn the previous steps we create the version v0.0.1 for our Operator, we are going to create and publish a v0.0.2 version:\nFirst we create a new CSV version in the same channel we used for v0.0.1:\nmake bundle VERSION=0.0.2 CHANNELS=alpha DEFAULT_CHANNEL=alpha IMG=quay.io/$QUAY_USERNAME/reversewords-operator:v0.0.2 We need to tweak the ClusterServiceVersion a bit:\nConfigure proper installModes Add WATCH_NAMESPACE env var to the operator deployment Add an Icon to our Operator You can download the modified CSV here:\ncurl -Ls https://linuxera.org/integrating-operators-olm/reverse-words-operator.clusterserviceversion_v0.0.2.yaml -o ~/operators-projects/reverse-words-operator/bundle/manifests/reverse-words-operator.clusterserviceversion.yaml sed -i \u0026#34;s/QUAY_USER/$QUAY_USERNAME/g\u0026#34; ~/operators-projects/reverse-words-operator/bundle/manifests/reverse-words-operator.clusterserviceversion.yaml Now that we have the new Operator Bundle ready we can build it and push it to Quay. After that we will update and build the index image.\nBuild the new bundle\npodman build -f bundle.Dockerfile -t quay.io/$QUAY_USERNAME/reversewords-operator-bundle:v0.0.2 Push and validate the new bundle\npodman push quay.io/$QUAY_USERNAME/reversewords-operator-bundle:v0.0.2 operator-sdk bundle validate quay.io/$QUAY_USERNAME/reversewords-operator-bundle:v0.0.2 -b podman Update the Index Image\n# Create the index image opm index add -c podman --bundles quay.io/$QUAY_USERNAME/reversewords-operator-bundle:v0.0.2 --from-index quay.io/$QUAY_USERNAME/reversewords-index:v0.0.1 --tag quay.io/$QUAY_USERNAME/reversewords-index:v0.0.2 # Push the index image podman push quay.io/$QUAY_USERNAME/reversewords-index:v0.0.2 With the index image updated, we can now update the CatalogSource pointing to the new index image:\nPATCH=\u0026#34;{\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;image\\\u0026#34;:\\\u0026#34;quay.io/$QUAY_USERNAME/reversewords-index:v0.0.2\\\u0026#34;}}\u0026#34; kubectl -n $OLM_NAMESPACE patch catalogsource reversewords-catalog -p $PATCH --type merge The catalog pod will be recreated with the new index image and the package manifest will be updated to include the CSV v0.0.2.\nUpdating to a new Operator version Depending on the installPlanApproval you selected when you created the subscription the operator will be updated automatically when a new version is published or you may need to approve the installPlan so the operator gets updated.\nSources OLM Generation Docs Operator Registry Docs Operator SDK OLM Integration Bundle Quickstart Operator SDK Generating Manifests and Metadata ","permalink":"https://linuxera.org/integrating-operators-olm/","summary":"Introduction This post is a continuation of our previous blog Writing Operators using the Operator Framework SDK.\nWe will continue working on the operator created on the previous blog, if you want to be able to follow this blog, you will need to run the steps from the previous blog.\nOperator Lifecycle Manager The Operator Lifecycle Manager is an open source toolkit to manage Operators in an effective, automated and scalable way.","title":"Integrating our Operators with OLM"},{"content":"Instrumenting your Applications We usually see systems being monitored by Ops teams, in fact, there are lots of valuable metrics that help Ops teams understand how the infrastructure they are managing is doing, but when it comes to applications monitoring, we don\u0026rsquo;t see those being monitored that carefully most of the time. Sometimes that ends up in application crashes that might be prevented with a proper monitoring strategy.\nIn this blog post we are going to see how we can instrument our applications using Prometheus metrics libraries. Prometheus metrics libraries are widely adopted, the Prometheus metrics format has become an independent project, OpenMetrics. OpenMetrics is trying to take Prometheus Metrics Format to the next level making it an industry standard.\nCustom Metrics Example In this example we are going to use our Simple Go Application as a reference.\nOur example application is capable of:\nReverses a word sent via POST on / endpoint Returns a release version set via an env var on / endpoint Returns the hostname of the machine where it\u0026rsquo;s running on /hostname endpoint Returns the app status on /health endpoint We are going to add the following metrics to our application:\nTotal number of words that have been reversed by our application Total number of times that a given endpoint has been accessed NOTE: This is an example application with example metrics, you should think carefully of which metrics do you want to include in your production applications.\nPrometheus Client The Prometheus Client is available for multiple programming languages, for today\u0026rsquo;s blog post we will be using the Go Client.\nThe Prometheus Client provides some metrics enabled by default, among those metrics we can find metrics related to memory consumption, cpu consumption, etc.\nEnable Prometheus Metrics Endpoint NOTE: Make sure you\u0026rsquo;re following metrics name best practices when defining your metrics.\nFirst, we need to import some required modules:\n\u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; Define reversewords_reversed_words_total metric. This metric is a simple counter metric.\nvar ( totalWordsReversed = prometheus.NewCounter( prometheus.CounterOpts{ Name: \u0026#34;reversewords_reversed_words_total\u0026#34;, Help: \u0026#34;Total number of reversed words\u0026#34;, }, ) ) Define reversewords_endpoints_accessed_total metric. This metric is a vector counter metric.\nvar ( endpointsAccessed = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;reversewords_endpoints_accessed_total\u0026#34;, Help: \u0026#34;Total number of accessed to a given endpoint\u0026#34;, }, []string{\u0026#34;accessed_endpoint\u0026#34;}, ) ) Our application has 4 different endpoints as we already seen before, these are the endpoints:\nrouter.HandleFunc(\u0026#34;/\u0026#34;, ReverseWord).Methods(\u0026#34;POST\u0026#34;) router.HandleFunc(\u0026#34;/\u0026#34;, ReturnRelease).Methods(\u0026#34;GET\u0026#34;) router.HandleFunc(\u0026#34;/hostname\u0026#34;, ReturnHostname).Methods(\u0026#34;GET\u0026#34;) router.HandleFunc(\u0026#34;/health\u0026#34;, ReturnHealth).Methods(\u0026#34;GET\u0026#34;) The reversewords_reversed_words_total metric will be increased every time the function ReverseWord is called:\nfunc ReverseWord(w http.ResponseWriter, r *http.Request) { \u0026lt;OUTPUT_OMITTED\u0026gt; totalWordsReversed.Inc() \u0026lt;OUTPUT_OMITTED\u0026gt; } The reversewords_endpoints_accessed_total metric will be increased every time the functions ReverseWord, ReturnRelease, ReturnHostname or ReturnHealth are called:\nfunc ReturnRelease(w http.ResponseWriter, r *http.Request) { \u0026lt;OUTPUT_OMITTED\u0026gt; endpointsAccessed.WithLabelValues(\u0026#34;release\u0026#34;).Inc() } func ReturnHostname(w http.ResponseWriter, r *http.Request) { \u0026lt;OUTPUT_OMITTED\u0026gt; endpointsAccessed.WithLabelValues(\u0026#34;hostname\u0026#34;).Inc() } func ReturnHealth(w http.ResponseWriter, r *http.Request) { \u0026lt;OUTPUT_OMITTED\u0026gt; endpointsAccessed.WithLabelValues(\u0026#34;health\u0026#34;).Inc() } func ReverseWord(w http.ResponseWriter, r *http.Request) { \u0026lt;OUTPUT_OMITTED\u0026gt; endpointsAccessed.WithLabelValues(\u0026#34;reverseword\u0026#34;).Inc() } Finally, we need to add the /metrics endpoint to our application:\nrouter.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()).Methods(\u0026#34;GET\u0026#34;) Gathering Metrics With our application running we can send a GET request to the /metrics endpoints to get the metrics:\n$ curl http://127.0.0.1:8080/metrics # HELP go_gc_duration_seconds A summary of the GC invocation durations. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\u0026#34;0\u0026#34;} 0 go_gc_duration_seconds{quantile=\u0026#34;0.25\u0026#34;} 0 go_gc_duration_seconds{quantile=\u0026#34;0.5\u0026#34;} 0 go_gc_duration_seconds{quantile=\u0026#34;0.75\u0026#34;} 0 go_gc_duration_seconds{quantile=\u0026#34;1\u0026#34;} 0 go_gc_duration_seconds_sum 0 go_gc_duration_seconds_count 0 # HELP go_goroutines Number of goroutines that currently exist. # TYPE go_goroutines gauge go_goroutines 8 # HELP go_info Information about the Go environment. # TYPE go_info gauge go_info{version=\u0026#34;go1.12.6\u0026#34;} 1 # HELP go_memstats_alloc_bytes Number of bytes allocated and still in use. # TYPE go_memstats_alloc_bytes gauge go_memstats_alloc_bytes 457776 # HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed. # TYPE go_memstats_alloc_bytes_total counter go_memstats_alloc_bytes_total 457776 # HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table. # TYPE go_memstats_buck_hash_sys_bytes gauge go_memstats_buck_hash_sys_bytes 2684 # HELP go_memstats_frees_total Total number of frees. # TYPE go_memstats_frees_total counter go_memstats_frees_total 172 # HELP go_memstats_gc_cpu_fraction The fraction of this program\u0026#39;s available CPU time used by the GC since the program started. # TYPE go_memstats_gc_cpu_fraction gauge go_memstats_gc_cpu_fraction 0 # HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata. # TYPE go_memstats_gc_sys_bytes gauge go_memstats_gc_sys_bytes 2.240512e+06 # HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use. # TYPE go_memstats_heap_alloc_bytes gauge go_memstats_heap_alloc_bytes 457776 # HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used. # TYPE go_memstats_heap_idle_bytes gauge go_memstats_heap_idle_bytes 6.5347584e+07 # HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use. # TYPE go_memstats_heap_inuse_bytes gauge go_memstats_heap_inuse_bytes 1.368064e+06 # HELP go_memstats_heap_objects Number of allocated objects. # TYPE go_memstats_heap_objects gauge go_memstats_heap_objects 2079 # HELP go_memstats_heap_released_bytes Number of heap bytes released to OS. # TYPE go_memstats_heap_released_bytes gauge go_memstats_heap_released_bytes 0 # HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system. # TYPE go_memstats_heap_sys_bytes gauge go_memstats_heap_sys_bytes 6.6715648e+07 # HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection. # TYPE go_memstats_last_gc_time_seconds gauge go_memstats_last_gc_time_seconds 0 # HELP go_memstats_lookups_total Total number of pointer lookups. # TYPE go_memstats_lookups_total counter go_memstats_lookups_total 0 # HELP go_memstats_mallocs_total Total number of mallocs. # TYPE go_memstats_mallocs_total counter go_memstats_mallocs_total 2251 # HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures. # TYPE go_memstats_mcache_inuse_bytes gauge go_memstats_mcache_inuse_bytes 6944 # HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system. # TYPE go_memstats_mcache_sys_bytes gauge go_memstats_mcache_sys_bytes 16384 # HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures. # TYPE go_memstats_mspan_inuse_bytes gauge go_memstats_mspan_inuse_bytes 19440 # HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system. # TYPE go_memstats_mspan_sys_bytes gauge go_memstats_mspan_sys_bytes 32768 # HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place. # TYPE go_memstats_next_gc_bytes gauge go_memstats_next_gc_bytes 4.473924e+06 # HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations. # TYPE go_memstats_other_sys_bytes gauge go_memstats_other_sys_bytes 527748 # HELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator. # TYPE go_memstats_stack_inuse_bytes gauge go_memstats_stack_inuse_bytes 393216 # HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator. # TYPE go_memstats_stack_sys_bytes gauge go_memstats_stack_sys_bytes 393216 # HELP go_memstats_sys_bytes Number of bytes obtained from system. # TYPE go_memstats_sys_bytes gauge go_memstats_sys_bytes 6.992896e+07 # HELP go_threads Number of OS threads created. # TYPE go_threads gauge go_threads 8 # HELP process_cpu_seconds_total Total user and system CPU time spent in seconds. # TYPE process_cpu_seconds_total counter process_cpu_seconds_total 0 # HELP process_max_fds Maximum number of open file descriptors. # TYPE process_max_fds gauge process_max_fds 1024 # HELP process_open_fds Number of open file descriptors. # TYPE process_open_fds gauge process_open_fds 7 # HELP process_resident_memory_bytes Resident memory size in bytes. # TYPE process_resident_memory_bytes gauge process_resident_memory_bytes 7.548928e+06 # HELP process_start_time_seconds Start time of the process since unix epoch in seconds. # TYPE process_start_time_seconds gauge process_start_time_seconds 1.56553026534e+09 # HELP process_virtual_memory_bytes Virtual memory size in bytes. # TYPE process_virtual_memory_bytes gauge process_virtual_memory_bytes 5.00559872e+08 # HELP process_virtual_memory_max_bytes Maximum amount of virtual memory available in bytes. # TYPE process_virtual_memory_max_bytes gauge process_virtual_memory_max_bytes -1 # HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served. # TYPE promhttp_metric_handler_requests_in_flight gauge promhttp_metric_handler_requests_in_flight 1 # HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code. # TYPE promhttp_metric_handler_requests_total counter promhttp_metric_handler_requests_total{code=\u0026#34;200\u0026#34;} 0 promhttp_metric_handler_requests_total{code=\u0026#34;500\u0026#34;} 0 promhttp_metric_handler_requests_total{code=\u0026#34;503\u0026#34;} 0 # HELP reversewords_reversed_words_total Total number of reversed words # TYPE reversewords_reversed_words_total counter reversewords_reversed_words_total 0 Now, let\u0026rsquo;s see how our metrics increase as we use our application:\n$ curl -s http://127.0.0.1:8080/ -X POST -d \u0026#39;{\u0026#34;word\u0026#34;:\u0026#34;PALC\u0026#34;}\u0026#39; {\u0026#34;reverse_word\u0026#34;:\u0026#34;CLAP\u0026#34;} $ curl -s http://127.0.0.1:8080/health Healthy $ curl -s http://127.0.0.1:8080/hostname Hostname: reverse-words-22j33j $ curl -s http://127.0.0.1:8080/metrics | grep \u0026#34;reversewords_\u0026#34; # HELP reversewords_endpoints_accessed_total Total number of accessed to a given endpoint # TYPE reversewords_endpoints_accessed_total counter reversewords_endpoints_accessed_total{accessed_endpoint=\u0026#34;health\u0026#34;} 1 reversewords_endpoints_accessed_total{accessed_endpoint=\u0026#34;hostname\u0026#34;} 1 reversewords_endpoints_accessed_total{accessed_endpoint=\u0026#34;reverseword\u0026#34;} 1 # HELP reversewords_reversed_words_total Total number of reversed words # TYPE reversewords_reversed_words_total counter reversewords_reversed_words_total 1 As you can see the reversewords_reversed_words_total has increased by 1 and the reversewords_endpoints_accessed_total metrics now show the total number of times a given endpoint has been accessed.\nNext Steps In a future blog post we are going to show how we can configure Prometheus to scrape our metrics endpoint and how Grafana can help us to create graphs that can be consumed by monitoring teams.\nUseful Resources If you want to learn more, feel free to take a look at the resources below.\nhttps://sysdig.com/blog/prometheus-metrics/ https://prometheus.io/docs/guides/go-application/ ","permalink":"https://linuxera.org/prometheus-metrics-on-your-applications/","summary":"Instrumenting your Applications We usually see systems being monitored by Ops teams, in fact, there are lots of valuable metrics that help Ops teams understand how the infrastructure they are managing is doing, but when it comes to applications monitoring, we don\u0026rsquo;t see those being monitored that carefully most of the time. Sometimes that ends up in application crashes that might be prevented with a proper monitoring strategy.\nIn this blog post we are going to see how we can instrument our applications using Prometheus metrics libraries.","title":"Enabling Prometheus Metrics on your Applications"},{"content":"What is OAuth Proxy A reverse proxy and static file server that provides authentication and authorization to an OpenShift OAuth server or Kubernetes master supporting the 1.6+ remote authorization endpoints to validate access to content. It is intended for use withing OpenShift clusters to make it easy to run both end-user and infrastructure services that do not provider their own authentication.\n[Source]\nSecuring an Application with OAuth Proxy In this blog post we are going to deploy OAuth Proxy in front of a simple application.\nWe will go through the following scenarios:\nApplication deployed without OAuth Proxy Application + OAuth Proxy limiting access to authenticated users Application + OAuth Proxy limiting access to specific users After following these three scenarios you will be able to secure applications on OpenShift and Kubernetes using the OAuth Proxy.\nScenario 1 - Deploying the Application without OAuth Proxy Not a big deal, just a regular deployment.\nRequired files deployment.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: reverse-words labels: name: reverse-words spec: replicas: 1 selector: matchLabels: name: reverse-words template: metadata: labels: name: reverse-words spec: containers: - name: reverse-words image: quay.io/mavazque/reversewords:latest imagePullPolicy: Always ports: - name: reverse-words containerPort: 8080 protocol: TCP service.yaml\napiVersion: v1 kind: Service metadata: labels: name: reverse-words name: reverse-words spec: ports: - name: app port: 8080 protocol: TCP targetPort: reverse-words selector: name: reverse-words sessionAffinity: None type: ClusterIP Deploy oc create namespace reverse-words oc -n reverse-words create -f deployment.yaml oc -n reverse-words create -f service.yaml oc -n reverse-words create route edge reverse-words --service=reverse-words --port=app --insecure-policy=Redirect Now we should be able to reach our application without providing any authentication details.\ncurl -k https://$(oc -n reverse-words get route reverse-words -o jsonpath=\u0026#39;{.status.ingress[*].host}\u0026#39;) -X POST -d \u0026#39;{\u0026#34;word\u0026#34;: \u0026#34;PALC\u0026#34;}\u0026#39; {\u0026#34;reverse_word\u0026#34;:\u0026#34;CLAP\u0026#34;} Let\u0026rsquo;s go ahead and secure our application to be accessible only to authenticated users.\nScenario 2 - Limiting Access to Authenticated Users In order to use OAuth Proxy we need a couple of things:\nCreate a session Secret used by OAuth Proxy to encrypt the login cookie A ServiceAccount used by our application and annotated to redirect traffic to a given route to the OAuth Proxy TLS Certificates for be used by the proxy (We will leverage OpenShift TLS service serving certificate) Modify our Deployment to include OAuth Proxy container Modify our Service to include OAuth Proxy port and annotation for certificate creation Prerequisites Create the Secret\noc -n reverse-words create secret generic reversewords-proxy --from-literal=session_secret=$(head /dev/urandom | tr -dc A-Za-z0-9 | head -c43) Create and annotate the ServiceAccount\noc -n reverse-words create serviceaccount reversewords oc -n reverse-words annotate serviceaccount reversewords serviceaccounts.openshift.io/oauth-redirectreference.reversewords=\u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;OAuthRedirectReference\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;reference\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;Route\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;reverse-words-authenticated\u0026#34;}}\u0026#39; Modify the deployment\nNOTE: Below deployment points to the quay.io/openshift/origin-oauth-proxy:4.10 image, make sure to use the one matching your cluster version. You can find the available tags here.\ndeployment.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: reverse-words labels: name: reverse-words spec: replicas: 1 selector: matchLabels: name: reverse-words template: metadata: labels: name: reverse-words spec: containers: - name: reverse-words image: quay.io/mavazque/reversewords:latest imagePullPolicy: Always ports: - name: reverse-words containerPort: 8080 protocol: TCP - name: oauth-proxy args: - -provider=openshift - -https-address=:8888 - -http-address= - -email-domain=* - -upstream=http://localhost:8080 - -tls-cert=/etc/tls/private/tls.crt - -tls-key=/etc/tls/private/tls.key - -cookie-secret-file=/etc/proxy/secrets/session_secret - -openshift-service-account=reversewords - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt - -skip-auth-regex=^/metrics image: quay.io/openshift/origin-oauth-proxy:4.10 imagePullPolicy: IfNotPresent ports: - name: oauth-proxy containerPort: 8888 protocol: TCP volumeMounts: - mountPath: /etc/tls/private name: secret-reversewords-tls - mountPath: /etc/proxy/secrets name: secret-reversewords-proxy serviceAccountName: reversewords volumes: - name: secret-reversewords-tls secret: defaultMode: 420 secretName: reversewords-tls - name: secret-reversewords-proxy secret: defaultMode: 420 secretName: reversewords-proxy Modify the service\nservice.yaml\napiVersion: v1 kind: Service metadata: annotations: service.alpha.openshift.io/serving-cert-secret-name: reversewords-tls labels: name: reverse-words name: reverse-words spec: ports: - name: proxy port: 8888 protocol: TCP targetPort: oauth-proxy - name: app port: 8080 protocol: TCP targetPort: reverse-words selector: name: reverse-words sessionAffinity: None type: ClusterIP Deploy oc -n reverse-words apply -f service.yaml oc -n reverse-words apply -f deployment.yaml oc -n reverse-words create route reencrypt reverse-words-authenticated --service=reverse-words --port=proxy --insecure-policy=Redirect Now we should be able to reach our application, let\u0026rsquo;s see what happens when we try to access without providing any authentication details.\ncurl -k -I https://$(oc -n reverse-words get route reverse-words-authenticated -o jsonpath=\u0026#39;{.status.ingress[*].host}\u0026#39;) HTTP/1.1 403 Forbidden Set-Cookie: _oauth_proxy=; Path=/; Domain=reverse-words-authenticated-reverse-words.apps.okd.linuxlabs.org; Expires=Tue, 30 Jul 2019 15:08:22 GMT; HttpOnly; Secure Date: Tue, 30 Jul 2019 16:08:22 GMT Content-Type: text/html; charset=utf-8 Set-Cookie: 24c429aac95893475d1e8c1316adf60f=255a07dc5b1af1d2d01721678f463c09; path=/; HttpOnly; Secure Now we are going to access to our application using our browser and authenticating with a valid user:\nScenario 3 - Limiting Access to Specific Authenticated Users In this scenario we are going to modify the OAuth Proxy configuration so only users with access to the reverse-words Namespace can access the application.\nPrerequisites Modify the deployment. Add the line below to the oauth-proxy container arguments\noc -n reverse-words edit deployment reverse-words \u0026lt;OMITTED OUTPUT\u0026gt; - -openshift-service-account=reversewords - -openshift-sar={\u0026#34;resource\u0026#34;:\u0026#34;namespaces\u0026#34;,\u0026#34;resourceName\u0026#34;:\u0026#34;reverse-words\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;reverse-words\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;get\u0026#34;} \u0026lt;OMITTED OUTPUT\u0026gt; Deploy The deployment should be updated and the OAuth Proxy should be configured to allow access only to users with access to the reverse-words namespace.\nAs we did before, let\u0026rsquo;s try to access with user1 to our application:\nIt failed! That is because user1 does not have access to the reverse-words namespace, let\u0026rsquo;s grant access to user2 and try to login again.\noc -n reverse-words adm policy add-role-to-user view user2 Back on the browser:\nFinal Thoughts This is just a sneak peek of what OAuth Proxy can do, if you want to know more you can check the project\u0026rsquo;s repository here.\nKeep in mind that OAuth Proxy is not intended to replace your application authentication and authorization mechanisms, it is just another security layer on top of your applications.\n","permalink":"https://linuxera.org/oauth-proxy-secure-applications-openshift/","summary":"What is OAuth Proxy A reverse proxy and static file server that provides authentication and authorization to an OpenShift OAuth server or Kubernetes master supporting the 1.6+ remote authorization endpoints to validate access to content. It is intended for use withing OpenShift clusters to make it easy to run both end-user and infrastructure services that do not provider their own authentication.\n[Source]\nSecuring an Application with OAuth Proxy In this blog post we are going to deploy OAuth Proxy in front of a simple application.","title":"Using OpenShift OAuth Proxy to secure your Applications on OpenShift"},{"content":"Operators, operators everywhere As you may have noticed, Kubernetes operators are becoming more an more popular those days. In this post we are going to explain the basics around Operators and we will develop a simple Operator using the Operator Framework SDK.\nWhat is an Operator An operator aims to automate actions usually performed manually while lessening the likelihood of error and simplifying complexity.\nWe can think of an operator as a method of packaging, deploying and managing a Kubernetes enabled application. Kubernetes enabled applications are deployed on Kubernetes and managed using the Kubernetes APIs and tooling.\nKubernetes APIs can be extended in order to enable new types of Kubernetes enabled applications. We could say that Operators are the runtime that manages such applications.\nA simple Operator would define how to deploy an application, whereas an advanced one will also take care of day-2 operations like backup, upgrades, etc.\nOperators use the Controller pattern, but not all Controllers are Operators. We could say it\u0026rsquo;s an Operator if it\u0026rsquo;s got:\nController Pattern API Extension Single-App Focus Feel free to read more about operators on the Operator FAQ by CoreOS\nKubernetes Controllers In the Kubernetes world, the Controllers take care of routine tasks to ensure cluster\u0026rsquo;s observed state, matches cluster\u0026rsquo;s desired state.\nEach Controller is responsible for a particular resource in Kubernetes. The Controller runs a control loop that watches the shared state of the cluster through the Kubernetes API server and makes changes attempting to move the current state towards the desired state.\nSome examples:\nReplication Controller Cronjob Controller Controller Components There are two main components in a controller: Informer/SharedInformer and WorkQueue.\nInformer In order to retrieve information about an object, the Controller sends a request to the Kubernetes API server. However, querying the API repeatedly can become expensive when dealing with thousands of objects.\nOn top of that, the Controller doesn\u0026rsquo;t really need to send requests continuously. It only cares about CRUD events happening on the objects it\u0026rsquo;s managing.\nInformers are not much used in the current Kubernetes, instead SharedInformers are used.\nSharedInformer A Informer creates a local cache for a set of resources used by itself. In Kubernetes there are multiple controllers running and caring about multiple kinds of resources though.\nHaving a shared cache among Controllers instead of one cache for each Controller sounds like a plan, that\u0026rsquo;s a SharedInformer.\nWorkQueue The SharedInformer can\u0026rsquo;t track what each Controller is up to, so the Controller must provide its own queuing and retrying mechanism.\nWhenever a resource changes, the SharedInformer\u0026rsquo;s Event Handler puts a key into the WorkQueue so the Controller will take care of that change.\nHow a Controller Works Control Loop Every controller has a Control Loop which basically does:\nProcesses every single item from the WorkQueue Pops an item and do whatever it needs to do with that item Pushes the item back to the WorkQueue if required Updates the item status to reflect the new changes Starts over Code Examples\nhttps://github.com/kubernetes/sample-controller/blob/release-1.18/controller.go#L180 https://github.com/kubernetes/sample-controller/blob/release-1.18/controller.go#L187 WorkQueue Stuff is put into the WorkQueue Stuff is take out from the WorkQueue in the Control Loop WorkQueue doesn\u0026rsquo;t store objects, it stores MetaNamespaceKeys A MetaNamespaceKey is a key-value reference for an object. It has the namespace for the resource and the name for the resource.\nCode Examples\nhttps://github.com/kubernetes/sample-controller/blob/release-1.18/controller.go#L111 https://github.com/kubernetes/sample-controller/blob/release-1.18/controller.go#L187 SharedInformer As we said before, is a shared data cache which distributes the data to all the Listers interested in knowing about changes happening to specific objects.\nThe most important part of the SharedInformer are the EventHandlers. Using an EventHandler is how you register your interest in specific object updates like addition, creation, updation or deletion.\nWhen an update occurs, the object will be put into the WorkQueue so it gets processed by the Controller in the Control Loop.\nListers are an important part of the SharedInformers as well. Listers are designed specifically to be used within Controllers as they have access to the cache.\nListers vs Client-go\nListers have access to the cache whereas Client-go will hit the Kubernetes API server (which is expensive when dealing with thousands of objects).\nCode Examples\nhttps://github.com/kubernetes/sample-controller/blob/release-1.18/controller.go#L252 https://github.com/kubernetes/sample-controller/blob/release-1.18/controller.go#L274 SyncHandler A.K.A Reconciliation Loop The first invocation of the SyncHandler will always be getting the MetaNamespaceKey for the resource it needs to work with.\nWith the MetaNamespaceKey the object is gathered from the cache, but well.. it\u0026rsquo;s not really an object, but a pointer to the cached object.\nWith the object reference we can read the object, in case the object needs to be updated, then the object have to be DeepCopied. DeepCopy is an expensive operation, making sure the object will be modified before calling DeepCopy is a good practice.\nWith the object reference / DeepCopy we are ready to apply our business logic.\nCode Examples\nhttps://github.com/kubernetes/sample-controller/blob/release-1.18/controller.go#L243 Kubernetes Controllers Some information about controllers:\nCronjob controller is probably the smallest one out there Sample Controller will help you getting started with Kubernetes Controllers Writing your very first Operator using the Operator Framework SDK We will create a very simple Operator using the Operator Framework SDK.\nThe Operator will be in charge of deploying a simple GoLang application.\nRequirements At the moment of this writing the following versions were used:\ngolang-1.18.7 Operator Framework SDK v1.24.1 Kubernetes 1.24 Installing the Operator Framework SDK RELEASE_VERSION=v1.24.1 # Linux sudo curl -L https://github.com/operator-framework/operator-sdk/releases/download/${RELEASE_VERSION}/operator-sdk_linux_amd64 -o /usr/local/bin/operator-sdk sudo chmod +x /usr/local/bin/operator-sdk Initializing the Operator Project First, a new new project for our Operator will be initialized.\nmkdir -p ~/operators-projects/reverse-words-operator \u0026amp;\u0026amp; cd $_ export GO111MODULE=on export GOPROXY=https://proxy.golang.org export GH_USER=\u0026lt;github_user\u0026gt; operator-sdk init --domain=linuxera.org --repo=github.com/$GH_USER/reverse-words-operator Create the Operator API Types As previously discussed, Operators extend the Kubernetes API, the API itself is organized in groups and versions. Our Operator will define a new Group, object Kind and its versioning.\nIn the example below we will define a new API Group called apps under domain linuxera.org, a new object Kind ReverseWordsApp and its versioning v1alpha1.\noperator-sdk create api --group=apps --version=v1alpha1 --kind=ReverseWordsApp --resource=true --controller=true Now it\u0026rsquo;s time to define the structure of our new Object. The Spec properties that we will be using are:\nreplicas: Will be used to define the number of replicas for our application appVersion: Will be used to define which version of the application is deployed In the Status we will use:\nappPods: Will track the pods associated to our current ReverseWordsApp instance Different conditions Below the code for our Types:\n/* Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package v1alpha1 import ( metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; ) // ReverseWordsAppSpec defines the desired state of ReverseWordsApp type ReverseWordsAppSpec struct { Replicas int32 `json:\u0026#34;replicas\u0026#34;` AppVersion string `json:\u0026#34;appVersion,omitempty\u0026#34;` } // ReverseWordsAppStatus defines the observed state of ReverseWordsApp type ReverseWordsAppStatus struct { AppPods []string `json:\u0026#34;appPods\u0026#34;` Conditions []metav1.Condition `json:\u0026#34;conditions\u0026#34;` } // +kubebuilder:object:root=true // +kubebuilder:subresource:status // ReverseWordsApp is the Schema for the reversewordsapps API type ReverseWordsApp struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec ReverseWordsAppSpec `json:\u0026#34;spec,omitempty\u0026#34;` Status ReverseWordsAppStatus `json:\u0026#34;status,omitempty\u0026#34;` } // +kubebuilder:object:root=true // ReverseWordsAppList contains a list of ReverseWordsApp type ReverseWordsAppList struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ListMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Items []ReverseWordsApp `json:\u0026#34;items\u0026#34;` } func init() { SchemeBuilder.Register(\u0026amp;ReverseWordsApp{}, \u0026amp;ReverseWordsAppList{}) } // Conditions const ( // ConditionTypeReverseWordsDeploymentNotReady indicates if the Reverse Words Deployment is not ready ConditionTypeReverseWordsDeploymentNotReady string = \u0026#34;ReverseWordsDeploymentNotReady\u0026#34; // ConditionTypeReady indicates if the Reverse Words Deployment is ready ConditionTypeReady string = \u0026#34;Ready\u0026#34; ) You can download the Types file:\ncurl -Ls https://linuxera.org/writing-operators-using-operator-framework/reversewordsapp_types.go -o ~/operators-projects/reverse-words-operator/api/v1alpha1/reversewordsapp_types.go Replicas will be defined as an int32 and will reference the Spec property replicas. For the status AppPods will be defined as a stringList and will reference the Status property appPods.\nWith above changes in-place we need to add new dependencies and re-generate some boilerplate code to take into account the latest changes in our types.\ngo mod tidy make manifests make generate Code your Operator business logic An empty controller (well, not that empty) has been created into our project, now it\u0026rsquo;s time to modify it so it actually deploys our application the way we want.\nOur application consists of a Deployment and a Service, so our Operator will deploy the Reverse Words App as follows:\nA Kubernetes Deployment object will be created A Kubernetes Service object will be created Below code (commented) for our Controller:\n/* Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package controllers import ( \u0026#34;context\u0026#34; \u0026#34;reflect\u0026#34; \u0026#34;github.com/go-logr/logr\u0026#34; appsv1alpha1 \u0026#34;github.com/mvazquezc/reverse-words-operator/api/v1alpha1\u0026#34; appsv1 \u0026#34;k8s.io/api/apps/v1\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/api/errors\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/api/meta\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/runtime\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/types\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/util/intstr\u0026#34; ctrl \u0026#34;sigs.k8s.io/controller-runtime\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/client\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/controller\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/controller/controllerutil\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/event\u0026#34; ctrllog \u0026#34;sigs.k8s.io/controller-runtime/pkg/log\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/predicate\u0026#34; ) // ReverseWordsAppReconciler reconciles a ReverseWordsApp object type ReverseWordsAppReconciler struct { client.Client Scheme *runtime.Scheme } const ( // Finalizer for our objects reverseWordsAppFinalizer = \u0026#34;finalizer.reversewordsapp.apps.linuxera.org\u0026#34; concurrentReconciles = 10 ) // +kubebuilder:rbac:groups=apps.linuxera.org,resources=reversewordsapps,verbs=get;list;watch;create;update;patch;delete // +kubebuilder:rbac:groups=apps.linuxera.org,resources=reversewordsapps/status,verbs=get;update;patch // +kubebuilder:rbac:groups=apps.linuxera.org,resources=reversewordsapps/finalizers,verbs=get;update;patch // +kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;create;update;patch;delete // +kubebuilder:rbac:groups=core,resources=services,verbs=get;list;watch;create;update;patch;delete // +kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch // +kubebuilder:rbac:groups=core,resources=events,verbs=create;patch func (r *ReverseWordsAppReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { log := ctrllog.FromContext(ctx) // Fetch the ReverseWordsApp instance instance := \u0026amp;appsv1alpha1.ReverseWordsApp{} err := r.Get(ctx, req.NamespacedName, instance) if err != nil { if errors.IsNotFound(err) { // Request object not found, could have been deleted after reconcile request. // Owned objects are automatically garbage collected. For additional cleanup logic use finalizers. // Return and don\u0026#39;t requeue log.Info(\u0026#34;ReverseWordsApp resource not found. Ignoring since object must be deleted\u0026#34;) return ctrl.Result{}, nil } // Error reading the object - requeue the request. log.Error(err, \u0026#34;Failed to get ReverseWordsApp\u0026#34;) return ctrl.Result{}, err } // Check if the CR is marked to be deleted isInstanceMarkedToBeDeleted := instance.GetDeletionTimestamp() != nil if isInstanceMarkedToBeDeleted { log.Info(\u0026#34;Instance marked for deletion, running finalizers\u0026#34;) if contains(instance.GetFinalizers(), reverseWordsAppFinalizer) { // Run the finalizer logic err := r.finalizeReverseWordsApp(log, instance) if err != nil { // Don\u0026#39;t remove the finalizer if we failed to finalize the object return ctrl.Result{}, err } log.Info(\u0026#34;Instance finalizers completed\u0026#34;) // Remove finalizer once the finalizer logic has run controllerutil.RemoveFinalizer(instance, reverseWordsAppFinalizer) err = r.Update(ctx, instance) if err != nil { // If the object update fails, requeue return ctrl.Result{}, err } } log.Info(\u0026#34;Instance can be deleted now\u0026#34;) return ctrl.Result{}, nil } // Add Finalizers to the CR if !contains(instance.GetFinalizers(), reverseWordsAppFinalizer) { if err := r.addFinalizer(log, instance); err != nil { return ctrl.Result{}, err } } // Reconcile Deployment object result, err := r.reconcileDeployment(instance, log) if err != nil { return result, err } // Reconcile Service object result, err = r.reconcileService(instance, log) if err != nil { return result, err } // The CR status is updated in the Deployment reconcile method return ctrl.Result{}, nil } func (r *ReverseWordsAppReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026amp;appsv1alpha1.ReverseWordsApp{}). Owns(\u0026amp;appsv1.Deployment{}). Owns(\u0026amp;corev1.Service{}). WithEventFilter(ignoreDeletionPredicate()). // Filter events that do not increase generation id, like status updates WithOptions(controller.Options{MaxConcurrentReconciles: concurrentReconciles}). // run multiple reconcile loops in parallel Complete(r) } func (r *ReverseWordsAppReconciler) reconcileDeployment(cr *appsv1alpha1.ReverseWordsApp, log logr.Logger) (ctrl.Result, error) { // Define a new Deployment object deployment := newDeploymentForCR(cr) // Set ReverseWordsApp instance as the owner and controller of the Deployment if err := ctrl.SetControllerReference(cr, deployment, r.Scheme); err != nil { return ctrl.Result{}, err } // Check if this Deployment already exists deploymentFound := \u0026amp;appsv1.Deployment{} err := r.Get(context.Background(), types.NamespacedName{Name: deployment.Name, Namespace: deployment.Namespace}, deploymentFound) if err != nil \u0026amp;\u0026amp; errors.IsNotFound(err) { log.Info(\u0026#34;Creating a new Deployment\u0026#34;, \u0026#34;Deployment.Namespace\u0026#34;, deployment.Namespace, \u0026#34;Deployment.Name\u0026#34;, deployment.Name) err = r.Create(context.Background(), deployment) if err != nil { return ctrl.Result{}, err } // Requeue the object to update its status return ctrl.Result{Requeue: true}, nil } else if err != nil { return ctrl.Result{}, err } else { // Deployment already exists log.Info(\u0026#34;Deployment already exists\u0026#34;, \u0026#34;Deployment.Namespace\u0026#34;, deploymentFound.Namespace, \u0026#34;Deployment.Name\u0026#34;, deploymentFound.Name) } // Ensure deployment replicas match the desired state if !reflect.DeepEqual(deploymentFound.Spec.Replicas, deployment.Spec.Replicas) { log.Info(\u0026#34;Current deployment replicas do not match ReverseWordsApp configured Replicas\u0026#34;) // Update the replicas err = r.Update(context.Background(), deployment) if err != nil { log.Error(err, \u0026#34;Failed to update Deployment.\u0026#34;, \u0026#34;Deployment.Namespace\u0026#34;, deploymentFound.Namespace, \u0026#34;Deployment.Name\u0026#34;, deploymentFound.Name) return ctrl.Result{}, err } } // Ensure deployment container image match the desired state, returns true if deployment needs to be updated if checkDeploymentImage(deploymentFound, deployment) { log.Info(\u0026#34;Current deployment image version do not match ReverseWordsApp configured version\u0026#34;) // Update the image err = r.Update(context.Background(), deployment) if err != nil { log.Error(err, \u0026#34;Failed to update Deployment.\u0026#34;, \u0026#34;Deployment.Namespace\u0026#34;, deploymentFound.Namespace, \u0026#34;Deployment.Name\u0026#34;, deploymentFound.Name) return ctrl.Result{}, err } } // Check if the deployment is ready deploymentReady := isDeploymentReady(deploymentFound) // Create list options for listing deployment pods podList := \u0026amp;corev1.PodList{} listOpts := []client.ListOption{ client.InNamespace(deploymentFound.Namespace), client.MatchingLabels(deploymentFound.Labels), } // List the pods for this ReverseWordsApp deployment err = r.List(context.Background(), podList, listOpts...) if err != nil { log.Error(err, \u0026#34;Failed to list Pods.\u0026#34;, \u0026#34;Deployment.Namespace\u0026#34;, deploymentFound.Namespace, \u0026#34;Deployment.Name\u0026#34;, deploymentFound.Name) return ctrl.Result{}, err } // Get running Pods from listing above (if any) podNames := getRunningPodNames(podList.Items) if deploymentReady { // Update the status to ready cr.Status.AppPods = podNames meta.SetStatusCondition(\u0026amp;cr.Status.Conditions, metav1.Condition{Type: appsv1alpha1.ConditionTypeReverseWordsDeploymentNotReady, Status: metav1.ConditionFalse, Reason: appsv1alpha1.ConditionTypeReverseWordsDeploymentNotReady}) meta.SetStatusCondition(\u0026amp;cr.Status.Conditions, metav1.Condition{Type: appsv1alpha1.ConditionTypeReady, Status: metav1.ConditionTrue, Reason: appsv1alpha1.ConditionTypeReady}) } else { // Update the status to not ready cr.Status.AppPods = podNames meta.SetStatusCondition(\u0026amp;cr.Status.Conditions, metav1.Condition{Type: appsv1alpha1.ConditionTypeReverseWordsDeploymentNotReady, Status: metav1.ConditionTrue, Reason: appsv1alpha1.ConditionTypeReverseWordsDeploymentNotReady}) meta.SetStatusCondition(\u0026amp;cr.Status.Conditions, metav1.Condition{Type: appsv1alpha1.ConditionTypeReady, Status: metav1.ConditionFalse, Reason: appsv1alpha1.ConditionTypeReady}) } // Reconcile the new status for the instance cr, err = r.updateReverseWordsAppStatus(cr, log) if err != nil { log.Error(err, \u0026#34;Failed to update ReverseWordsApp Status.\u0026#34;) return ctrl.Result{}, err } // Deployment reconcile finished return ctrl.Result{}, nil } // updateReverseWordsAppStatus updates the Status of a given CR func (r *ReverseWordsAppReconciler) updateReverseWordsAppStatus(cr *appsv1alpha1.ReverseWordsApp, log logr.Logger) (*appsv1alpha1.ReverseWordsApp, error) { reverseWordsApp := \u0026amp;appsv1alpha1.ReverseWordsApp{} err := r.Get(context.Background(), types.NamespacedName{Name: cr.Name, Namespace: cr.Namespace}, reverseWordsApp) if err != nil { return reverseWordsApp, err } if !reflect.DeepEqual(cr.Status, reverseWordsApp.Status) { log.Info(\u0026#34;Updating ReverseWordsApp Status.\u0026#34;) // We need to update the status err = r.Status().Update(context.Background(), cr) if err != nil { return cr, err } updatedReverseWordsApp := \u0026amp;appsv1alpha1.ReverseWordsApp{} err = r.Get(context.Background(), types.NamespacedName{Name: cr.Name, Namespace: cr.Namespace}, updatedReverseWordsApp) if err != nil { return cr, err } cr = updatedReverseWordsApp.DeepCopy() } return cr, nil } // addFinalizer adds a given finalizer to a given CR func (r *ReverseWordsAppReconciler) addFinalizer(log logr.Logger, cr *appsv1alpha1.ReverseWordsApp) error { log.Info(\u0026#34;Adding Finalizer for the ReverseWordsApp\u0026#34;) controllerutil.AddFinalizer(cr, reverseWordsAppFinalizer) // Update CR err := r.Update(context.Background(), cr) if err != nil { log.Error(err, \u0026#34;Failed to update ReverseWordsApp with finalizer\u0026#34;) return err } return nil } // finalizeReverseWordsApp runs required tasks before deleting the objects owned by the CR func (r *ReverseWordsAppReconciler) finalizeReverseWordsApp(log logr.Logger, cr *appsv1alpha1.ReverseWordsApp) error { // TODO(user): Add the cleanup steps that the operator // needs to do before the CR can be deleted. Examples // of finalizers include performing backups and deleting // resources that are not owned by this CR, like a PVC. log.Info(\u0026#34;Successfully finalized ReverseWordsApp\u0026#34;) return nil } func (r *ReverseWordsAppReconciler) reconcileService(cr *appsv1alpha1.ReverseWordsApp, log logr.Logger) (ctrl.Result, error) { // Define a new Service object service := newServiceForCR(cr) // Set ReverseWordsApp instance as the owner and controller of the Service if err := controllerutil.SetControllerReference(cr, service, r.Scheme); err != nil { return ctrl.Result{}, err } // Check if this Service already exists serviceFound := \u0026amp;corev1.Service{} err := r.Get(context.Background(), types.NamespacedName{Name: service.Name, Namespace: service.Namespace}, serviceFound) if err != nil \u0026amp;\u0026amp; errors.IsNotFound(err) { log.Info(\u0026#34;Creating a new Service\u0026#34;, \u0026#34;Service.Namespace\u0026#34;, service.Namespace, \u0026#34;Service.Name\u0026#34;, service.Name) err = r.Create(context.Background(), service) if err != nil { return ctrl.Result{}, err } // Service created successfully - don\u0026#39;t requeue return ctrl.Result{}, nil } else if err != nil { return ctrl.Result{}, err } else { // Service already exists log.Info(\u0026#34;Service already exists\u0026#34;, \u0026#34;Service.Namespace\u0026#34;, serviceFound.Namespace, \u0026#34;Service.Name\u0026#34;, serviceFound.Name) } // Service reconcile finished return ctrl.Result{}, nil } // Returns a new deployment without replicas configured // replicas will be configured in the sync loop func newDeploymentForCR(cr *appsv1alpha1.ReverseWordsApp) *appsv1.Deployment { labels := map[string]string{ \u0026#34;app\u0026#34;: cr.Name, } replicas := cr.Spec.Replicas // Minimum replicas will be 1 if replicas == 0 { replicas = 1 } appVersion := \u0026#34;latest\u0026#34; if cr.Spec.AppVersion != \u0026#34;\u0026#34; { appVersion = cr.Spec.AppVersion } // TODO:Check if application version exists containerImage := \u0026#34;quay.io/mavazque/reversewords:\u0026#34; + appVersion probe := \u0026amp;corev1.Probe{ ProbeHandler: corev1.ProbeHandler{ HTTPGet: \u0026amp;corev1.HTTPGetAction{ Path: \u0026#34;/health\u0026#34;, Port: intstr.FromInt(8080), }, }, InitialDelaySeconds: 5, TimeoutSeconds: 2, PeriodSeconds: 15, } return \u0026amp;appsv1.Deployment{ TypeMeta: metav1.TypeMeta{ APIVersion: \u0026#34;apps/v1\u0026#34;, Kind: \u0026#34;Deployment\u0026#34;, }, ObjectMeta: metav1.ObjectMeta{ Name: \u0026#34;dp-\u0026#34; + cr.Name, Namespace: cr.Namespace, Labels: labels, }, Spec: appsv1.DeploymentSpec{ Replicas: \u0026amp;replicas, Selector: \u0026amp;metav1.LabelSelector{ MatchLabels: labels, }, Template: corev1.PodTemplateSpec{ ObjectMeta: metav1.ObjectMeta{ Labels: labels, }, Spec: corev1.PodSpec{ Containers: []corev1.Container{ { Image: containerImage, Name: \u0026#34;reversewords\u0026#34;, Ports: []corev1.ContainerPort{ { ContainerPort: 8080, Name: \u0026#34;reversewords\u0026#34;, }, }, LivenessProbe: probe, ReadinessProbe: probe, }, }, }, }, }, } } // Returns a new service func newServiceForCR(cr *appsv1alpha1.ReverseWordsApp) *corev1.Service { labels := map[string]string{ \u0026#34;app\u0026#34;: cr.Name, } return \u0026amp;corev1.Service{ TypeMeta: metav1.TypeMeta{ APIVersion: \u0026#34;v1\u0026#34;, Kind: \u0026#34;Service\u0026#34;, }, ObjectMeta: metav1.ObjectMeta{ Name: \u0026#34;service-\u0026#34; + cr.Name, Namespace: cr.Namespace, Labels: labels, }, Spec: corev1.ServiceSpec{ Type: corev1.ServiceTypeLoadBalancer, Selector: labels, Ports: []corev1.ServicePort{ { Name: \u0026#34;http\u0026#34;, Port: 8080, }, }, }, } } // isDeploymentReady returns a true bool if the deployment has all its pods ready func isDeploymentReady(deployment *appsv1.Deployment) bool { configuredReplicas := deployment.Status.Replicas readyReplicas := deployment.Status.ReadyReplicas deploymentReady := false if configuredReplicas == readyReplicas { deploymentReady = true } return deploymentReady } // getRunningPodNames returns the pod names for the pods running in the array of pods passed in func getRunningPodNames(pods []corev1.Pod) []string { // Create an empty []string, so if no podNames are returned, instead of nil we get an empty slice var podNames []string = make([]string, 0) for _, pod := range pods { if pod.GetObjectMeta().GetDeletionTimestamp() != nil { continue } if pod.Status.Phase == corev1.PodPending || pod.Status.Phase == corev1.PodRunning { podNames = append(podNames, pod.Name) } } return podNames } // checkDeploymentImage returns wether the deployment image is different or not func checkDeploymentImage(current *appsv1.Deployment, desired *appsv1.Deployment) bool { for _, curr := range current.Spec.Template.Spec.Containers { for _, des := range desired.Spec.Template.Spec.Containers { // Only compare the images of containers with the same name if curr.Name == des.Name { if curr.Image != des.Image { return true } } } } return false } // contains returns true if a string is found on a slice func contains(list []string, s string) bool { for _, v := range list { if v == s { return true } } return false } // Ignore changes that do not increase the resource generation func ignoreDeletionPredicate() predicate.Predicate { return predicate.Funcs{ UpdateFunc: func(e event.UpdateEvent) bool { // Ignore updates to CR status in which case metadata.Generation does not change return e.ObjectOld.GetGeneration() != e.ObjectNew.GetGeneration() }, DeleteFunc: func(e event.DeleteEvent) bool { // Evaluates to false if the object has been confirmed deleted. return !e.DeleteStateUnknown }, } } You can download the controller code, remember to change the GitHub ID before bulding the operator:\n# Remember to change import: appsv1alpha1 \u0026#34;github.com/mvazquezc/reverse-words-operator/api/v1alpha1\u0026#34; curl -Ls https://linuxera.org/writing-operators-using-operator-framework/reversewordsapp_controller.go -o ~/operators-projects/reverse-words-operator/controllers/reversewordsapp_controller.go Setup Watch namespaces By default, the controller will watch all namespaces, in this case we want it to watch only the namespace where it runs, in order to do so we need to update the controller options in the main.go file.\n/* Copyright 2021. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; // Import all Kubernetes client auth plugins (e.g. Azure, GCP, OIDC, etc.) // to ensure that exec-entrypoint and run can make use of them. _ \u0026#34;k8s.io/client-go/plugin/pkg/client/auth\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/runtime\u0026#34; utilruntime \u0026#34;k8s.io/apimachinery/pkg/util/runtime\u0026#34; clientgoscheme \u0026#34;k8s.io/client-go/kubernetes/scheme\u0026#34; ctrl \u0026#34;sigs.k8s.io/controller-runtime\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/healthz\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/log/zap\u0026#34; appsv1alpha1 \u0026#34;github.com/mvazquezc/reverse-words-operator/api/v1alpha1\u0026#34; \u0026#34;github.com/mvazquezc/reverse-words-operator/controllers\u0026#34; //+kubebuilder:scaffold:imports ) var ( scheme = runtime.NewScheme() setupLog = ctrl.Log.WithName(\u0026#34;setup\u0026#34;) ) func init() { utilruntime.Must(clientgoscheme.AddToScheme(scheme)) utilruntime.Must(appsv1alpha1.AddToScheme(scheme)) //+kubebuilder:scaffold:scheme } func main() { var metricsAddr string var enableLeaderElection bool var probeAddr string flag.StringVar(\u0026amp;metricsAddr, \u0026#34;metrics-bind-address\u0026#34;, \u0026#34;:8080\u0026#34;, \u0026#34;The address the metric endpoint binds to.\u0026#34;) flag.StringVar(\u0026amp;probeAddr, \u0026#34;health-probe-bind-address\u0026#34;, \u0026#34;:8081\u0026#34;, \u0026#34;The address the probe endpoint binds to.\u0026#34;) flag.BoolVar(\u0026amp;enableLeaderElection, \u0026#34;leader-elect\u0026#34;, false, \u0026#34;Enable leader election for controller manager. \u0026#34;+ \u0026#34;Enabling this will ensure there is only one active controller manager.\u0026#34;) opts := zap.Options{ Development: true, } opts.BindFlags(flag.CommandLine) flag.Parse() ctrl.SetLogger(zap.New(zap.UseFlagOptions(\u0026amp;opts))) watchNamespace, err := getWatchNamespace() mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{ Scheme: scheme, MetricsBindAddress: metricsAddr, Port: 9443, HealthProbeBindAddress: probeAddr, LeaderElection: enableLeaderElection, LeaderElectionID: \u0026#34;1ef59d40.linuxera.org\u0026#34;, Namespace: watchNamespace, // namespaced-scope when the value is not an empty string }) if err != nil { setupLog.Error(err, \u0026#34;unable to start manager\u0026#34;) os.Exit(1) } if err = (\u0026amp;controllers.ReverseWordsAppReconciler{ Client: mgr.GetClient(), Scheme: mgr.GetScheme(), }).SetupWithManager(mgr); err != nil { setupLog.Error(err, \u0026#34;unable to create controller\u0026#34;, \u0026#34;controller\u0026#34;, \u0026#34;ReverseWordsApp\u0026#34;) os.Exit(1) } //+kubebuilder:scaffold:builder if err := mgr.AddHealthzCheck(\u0026#34;healthz\u0026#34;, healthz.Ping); err != nil { setupLog.Error(err, \u0026#34;unable to set up health check\u0026#34;) os.Exit(1) } if err := mgr.AddReadyzCheck(\u0026#34;readyz\u0026#34;, healthz.Ping); err != nil { setupLog.Error(err, \u0026#34;unable to set up ready check\u0026#34;) os.Exit(1) } setupLog.Info(\u0026#34;starting manager\u0026#34;) if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, \u0026#34;problem running manager\u0026#34;) os.Exit(1) } } // getWatchNamespace returns the Namespace the operator should be watching for changes func getWatchNamespace() (string, error) { // WatchNamespaceEnvVar is the constant for env variable WATCH_NAMESPACE // which specifies the Namespace to watch. // An empty value means the operator is running with cluster scope. var watchNamespaceEnvVar = \u0026#34;WATCH_NAMESPACE\u0026#34; ns, found := os.LookupEnv(watchNamespaceEnvVar) if !found { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;%s must be set\u0026#34;, watchNamespaceEnvVar) } return ns, nil } You can download the main.go, remember to change the GitHub ID before bulding the operator:\n# Remember to change import: appsv1alpha1 \u0026#34;github.com/mvazquezc/reverse-words-operator/api/v1alpha1\u0026#34; curl -Ls https://linuxera.org/writing-operators-using-operator-framework/main.go -o ~/operators-projects/reverse-words-operator/main.go Specify permissions and generate RBAC manifests Our controller needs some RBAC permissions to interact with the resources it manages. These has been specified via RBAC Markers in our controller code:\n// +kubebuilder:rbac:groups=apps.linuxera.org,resources=reversewordsapps,verbs=get;list;watch;create;update;patch;delete // +kubebuilder:rbac:groups=apps.linuxera.org,resources=reversewordsapps/status,verbs=get;update;patch // +kubebuilder:rbac:groups=apps.linuxera.org,resources=reversewordsapps/finalizers,verbs=get;update;patch // +kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;create;update;patch;delete // +kubebuilder:rbac:groups=core,resources=services,verbs=get;list;watch;create;update;patch;delete // +kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch // +kubebuilder:rbac:groups=core,resources=events,verbs=create;patch func (r *ReverseWordsAppReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) { The ClusterRole manifest at config/rbac/role.yaml is generated from the above markers via controller-gen with the following command:\ngo mod tidy make manifests Build the Operator First, we will build the operator and once the image is built, we will push it to the Quay Registry.\nBefore we start building the operator, we need access to a Kubernetes cluster. If you don\u0026rsquo;t have one you can use Kind, Minikube or my prefered one, KCli\nIn order to get a local cluster with KCli, just run this command:\nkcli create kube generic -P masters=1 -P workers=1 -P master_memory=4096 -P numcpus=2 -P worker_memory=4096 -P sdn=calico -P version=1.18 -P ingress=true -P ingress_method=nginx -P metallb=true -P domain=linuxera.org operatorscluster Now that we have the cluster up and running we will build and push the operator.\nNOTE: If you use podman instead of docker you can edit the Makefile and change docker commands by podman commands\nexport USERNAME=\u0026lt;quay-username\u0026gt; make docker-build docker-push IMG=quay.io/$USERNAME/reversewords-operator:v0.0.1 Deploy the Operator Create the required CRDs in the cluster\nmake install Deploy the operator\nNOTE: While developing you can run the operator locally (you need a valid kubeconfig) by running make run\nIn order to deploy the different operator pieces, Kustomize is used. There is a Kustomization file (~/operators-projects/reverse-words-operator/config/default/kustomization.yaml) where you can define some defaults for your operator, like the namePrefix for the different objects or the namespace where it will be deployed.\nEdit the default kustomization file ~/operators-projects/reverse-words-operator/config/default/kustomization.yaml and specify the namespace where your operator should run by modifying the namespace property\nexport NAMESPACE=operators-test sed -i \u0026#34;s/namespace: .*/namespace: $NAMESPACE/g\u0026#34; ~/operators-projects/reverse-words-operator/config/default/kustomization.yaml Create the namespace and Deploy the operator\nkubectl create ns $NAMESPACE export USERNAME=\u0026lt;quay_username\u0026gt; make deploy IMG=quay.io/$USERNAME/reversewords-operator:v0.0.1 Patch the controller deployment so it only watches the namespace where it\u0026rsquo;s running\nkubectl -n $NAMESPACE patch deployment reverse-words-operator-controller-manager -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;$setElementOrder/containers\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;kube-rbac-proxy\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;manager\u0026#34;}],\u0026#34;containers\u0026#34;:[{\u0026#34;env\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;WATCH_NAMESPACE\u0026#34;,\u0026#34;valueFrom\u0026#34;:{\u0026#34;fieldRef\u0026#34;:{\u0026#34;fieldPath\u0026#34;:\u0026#34;metadata.namespace\u0026#34;}}}],\u0026#34;name\u0026#34;:\u0026#34;manager\u0026#34;}]}}}}\u0026#39; We should see our operator pod up and running\n1.6710255038089378e+09\tINFO\tcontroller-runtime.metrics\tMetrics server is starting to listen\t{\u0026#34;addr\u0026#34;: \u0026#34;127.0.0.1:8080\u0026#34;} 1.671025503809387e+09\tINFO\tsetup\tstarting manager 1.6710255038099382e+09\tINFO\tStarting server\t{\u0026#34;kind\u0026#34;: \u0026#34;health probe\u0026#34;, \u0026#34;addr\u0026#34;: \u0026#34;[::]:8081\u0026#34;} I1214 13:45:03.809973 1 leaderelection.go:248] attempting to acquire leader lease operators-test/1ef59d40.linuxera.org... 1.6710255038100107e+09\tINFO\tStarting server\t{\u0026#34;path\u0026#34;: \u0026#34;/metrics\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;metrics\u0026#34;, \u0026#34;addr\u0026#34;: \u0026#34;127.0.0.1:8080\u0026#34;} I1214 13:45:21.937458 1 leaderelection.go:258] successfully acquired lease operators-test/1ef59d40.linuxera.org 1.6710255219376462e+09\tDEBUG\tevents\tNormal\t{\u0026#34;object\u0026#34;: {\u0026#34;kind\u0026#34;:\u0026#34;Lease\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;operators-test\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;1ef59d40.linuxera.org\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;33044f96-0031-4b1d-a77f-cac5a22a2368\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;coordination.k8s.io/v1\u0026#34;,\u0026#34;resourceVersion\u0026#34;:\u0026#34;172072640\u0026#34;}, \u0026#34;reason\u0026#34;: \u0026#34;LeaderElection\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;reverse-words-operator-controller-manager-6846c949f8-2mtm4_687c8c2a-921e-4d2b-a5c5-de2c4ae1709f became leader\u0026#34;} 1.6710255219378495e+09\tINFO\tStarting EventSource\t{\u0026#34;controller\u0026#34;: \u0026#34;reversewordsapp\u0026#34;, \u0026#34;controllerGroup\u0026#34;: \u0026#34;apps.linuxera.org\u0026#34;, \u0026#34;controllerKind\u0026#34;: \u0026#34;ReverseWordsApp\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;kind source: *v1alpha1.ReverseWordsApp\u0026#34;} 1.6710255219378843e+09\tINFO\tStarting EventSource\t{\u0026#34;controller\u0026#34;: \u0026#34;reversewordsapp\u0026#34;, \u0026#34;controllerGroup\u0026#34;: \u0026#34;apps.linuxera.org\u0026#34;, \u0026#34;controllerKind\u0026#34;: \u0026#34;ReverseWordsApp\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;kind source: *v1.Deployment\u0026#34;} 1.6710255219378898e+09\tINFO\tStarting EventSource\t{\u0026#34;controller\u0026#34;: \u0026#34;reversewordsapp\u0026#34;, \u0026#34;controllerGroup\u0026#34;: \u0026#34;apps.linuxera.org\u0026#34;, \u0026#34;controllerKind\u0026#34;: \u0026#34;ReverseWordsApp\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;kind source: *v1.Service\u0026#34;} 1.6710255219378932e+09\tINFO\tStarting Controller\t{\u0026#34;controller\u0026#34;: \u0026#34;reversewordsapp\u0026#34;, \u0026#34;controllerGroup\u0026#34;: \u0026#34;apps.linuxera.org\u0026#34;, \u0026#34;controllerKind\u0026#34;: \u0026#34;ReverseWordsApp\u0026#34;} 1.6710255220399954e+09\tINFO\tStarting workers\t{\u0026#34;controller\u0026#34;: \u0026#34;reversewordsapp\u0026#34;, \u0026#34;controllerGroup\u0026#34;: \u0026#34;apps.linuxera.org\u0026#34;, \u0026#34;controllerKind\u0026#34;: \u0026#34;ReverseWordsApp\u0026#34;, \u0026#34;worker count\u0026#34;: 10} Now it\u0026rsquo;s time to create ReverseWordsApp instances\ncat \u0026lt;\u0026lt;EOF | kubectl -n $NAMESPACE create -f - apiVersion: apps.linuxera.org/v1alpha1 kind: ReverseWordsApp metadata: name: example-reversewordsapp spec: replicas: 1 EOF cat \u0026lt;\u0026lt;EOF | kubectl -n $NAMESPACE create -f - apiVersion: apps.linuxera.org/v1alpha1 kind: ReverseWordsApp metadata: name: example-reversewordsapp-2 spec: replicas: 2 EOF We should see two deployments and services being created, and if wee look at the status of our object we should see the pods backing the instance\nkubectl -n $NAMESPACE get reversewordsapps example-reversewordsapp -o yaml apiVersion: apps.linuxera.org/v1alpha1 kind: ReverseWordsApp metadata: creationTimestamp: \u0026#34;2022-12-14T13:47:10Z\u0026#34; finalizers: - finalizer.reversewordsapp.apps.linuxera.org generation: 1 name: example-reversewordsapp namespace: operators-test resourceVersion: \u0026#34;172075873\u0026#34; uid: 1244e3fd-984d-4132-bdd7-bcfb70e502ff spec: replicas: 1 status: appPods: - dp-example-reversewordsapp-75cff95fd8-d2tl2 conditions: - lastTransitionTime: \u0026#34;2022-12-14T13:47:10Z\u0026#34; message: \u0026#34;\u0026#34; reason: ReverseWordsDeploymentNotReady status: \u0026#34;True\u0026#34; type: ReverseWordsDeploymentNotReady - lastTransitionTime: \u0026#34;2022-12-14T13:47:10Z\u0026#34; message: \u0026#34;\u0026#34; reason: Ready status: \u0026#34;False\u0026#34; type: Ready We can test our application now\nLB_ENDPOINT=$(kubectl -n $NAMESPACE get svc --selector=\u0026#39;app=example-reversewordsapp\u0026#39; -o jsonpath=\u0026#39;{.items[*].status.loadBalancer.ingress[*].ip}\u0026#39;) curl -X POST -d \u0026#39;{\u0026#34;word\u0026#34;:\u0026#34;PALC\u0026#34;}\u0026#39; http://$LB_ENDPOINT:8080 {\u0026#34;reverse_word\u0026#34;:\u0026#34;CLAP\u0026#34;} Cleanup\nkubectl -n $NAMESPACE delete reversewordsapp example-reversewordsapp example-reversewordsapp-2 kubectl delete -f config/crd/bases/apps.linuxera.org_reversewordsapps.yaml kubectl delete ns operators-test That\u0026rsquo;s it!\nIn the next episode: We will look at how to use OLM to release our operator We will see a K8s controllers deep dive Sources Operators by CoreOS A deep dive into Kubernetes Controllers Writing Kube Controllers for Everyone ","permalink":"https://linuxera.org/writing-operators-using-operator-framework/","summary":"Operators, operators everywhere As you may have noticed, Kubernetes operators are becoming more an more popular those days. In this post we are going to explain the basics around Operators and we will develop a simple Operator using the Operator Framework SDK.\nWhat is an Operator An operator aims to automate actions usually performed manually while lessening the likelihood of error and simplifying complexity.\nWe can think of an operator as a method of packaging, deploying and managing a Kubernetes enabled application.","title":"Writing Operators using the Operator Framework SDK"}]